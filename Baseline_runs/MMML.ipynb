{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_uUMCMJn9vXG",
    "outputId": "1e478b74-efac-474f-a3cf-e7ef7b4894c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mmf'...\n",
      "remote: Enumerating objects: 101, done.\u001b[K\n",
      "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
      "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
      "remote: Total 16225 (delta 34), reused 56 (delta 22), pack-reused 16124\u001b[K\n",
      "Receiving objects: 100% (16225/16225), 12.88 MiB | 0 bytes/s, done.\n",
      "Resolving deltas: 100% (10287/10287), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/mmf.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YkFN1slO_5Kk"
   },
   "outputs": [],
   "source": [
    "!cd mmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPKk2x5fAAdK",
    "outputId": "b55725f3-365a-4c4c-f4f2-9a1a28d18880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/ubuntu/mmf\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting sklearn==0.0\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Collecting torchtext==0.5.0\n",
      "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 3.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting fasttext==0.9.1\n",
      "  Using cached fasttext-0.9.1.tar.gz (57 kB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from mmf==1.0.0rc12) (1.18.1)\n",
      "Collecting tqdm>=4.43.0\n",
      "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting lmdb==0.98\n",
      "  Downloading lmdb-0.98.tar.gz (869 kB)\n",
      "\u001b[K     |████████████████████████████████| 869 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.23.0\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 11.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor==1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting GitPython==3.1.0\n",
      "  Downloading GitPython-3.1.0-py3-none-any.whl (450 kB)\n",
      "\u001b[K     |████████████████████████████████| 450 kB 78.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting editdistance==0.5.3\n",
      "  Downloading editdistance-0.5.3-cp36-cp36m-manylinux1_x86_64.whl (178 kB)\n",
      "\u001b[K     |████████████████████████████████| 178 kB 96.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.6.0\n",
      "  Downloading torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8 MB 5.6 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk==3.4.5 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from mmf==1.0.0rc12) (3.4.5)\n",
      "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'omegaconf' candidate (version 2.0.1rc4 at https://files.pythonhosted.org/packages/03/c6/dec84d1b2a3d645f03201dca03bc879b6116cb6503449a31d7ff9c1394a4/omegaconf-2.0.1rc4-py3-none-any.whl#sha256=e04462f7e3d8f51532221471b241f67e35a36a04e364c70987018faadd273cc0 (from https://pypi.org/simple/omegaconf/) (requires-python:>=3.6))\n",
      "Reason for being yanked: <none given>\u001b[0m\n",
      "Collecting omegaconf==2.0.1rc4\n",
      "  Using cached omegaconf-2.0.1rc4-py3-none-any.whl (34 kB)\n",
      "Collecting transformers==3.4.0\n",
      "  Downloading transformers-3.4.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 70.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision==0.7.0\n",
      "  Downloading torchvision-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.9 MB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting demjson==2.2.4\n",
      "  Using cached demjson-2.2.4.tar.gz (131 kB)\n",
      "Requirement already satisfied: scikit-learn in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sklearn==0.0->mmf==1.0.0rc12) (0.22.1)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 60.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.5.0->mmf==1.0.0rc12) (1.14.0)\n",
      "Requirement already satisfied: pybind11>=2.2 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (2.6.1)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from fasttext==0.9.1->mmf==1.0.0rc12) (45.2.0.post20200210)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests==2.23.0->mmf==1.0.0rc12) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests==2.23.0->mmf==1.0.0rc12) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests==2.23.0->mmf==1.0.0rc12) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests==2.23.0->mmf==1.0.0rc12) (3.0.4)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.5-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: future in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.6.0->mmf==1.0.0rc12) (0.18.2)\n",
      "Requirement already satisfied: PyYAML>=5.1.* in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from omegaconf==2.0.1rc4->mmf==1.0.0rc12) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from omegaconf==2.0.1rc4->mmf==1.0.0rc12) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses; python_version == \"3.6\" in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from omegaconf==2.0.1rc4->mmf==1.0.0rc12) (0.7)\n",
      "Requirement already satisfied: filelock in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.0.12)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2020.11.11-cp36-cp36m-manylinux2014_x86_64.whl (723 kB)\n",
      "\u001b[K     |████████████████████████████████| 723 kB 93.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 67.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.9.2\n",
      "  Downloading tokenizers-0.9.2-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 84.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==3.4.0->mmf==1.0.0rc12) (3.11.4)\n",
      "Requirement already satisfied: packaging in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==3.4.0->mmf==1.0.0rc12) (20.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision==0.7.0->mmf==1.0.0rc12) (7.0.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn->sklearn==0.0->mmf==1.0.0rc12) (0.14.1)\n",
      "Collecting smmap<4,>=3.0.1\n",
      "  Downloading smmap-3.0.4-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: click in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==3.4.0->mmf==1.0.0rc12) (7.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in ./anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers==3.4.0->mmf==1.0.0rc12) (2.4.6)\n",
      "Building wheels for collected packages: sklearn, fasttext, lmdb, termcolor, demjson, sacremoses\n",
      "  Building wheel for sklearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1315 sha256=25bf32f1ff168062553565ed4a54a9da1325721b12bfe579ef54e59ce7f5b3bd\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/23/9d/42/5ec745cbbb17517000a53cecc49d6a865450d1f5cb16dc8a9c\n",
      "  Building wheel for fasttext (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=216473 sha256=bafa39626b03e6b2e75a2aa61f69575a5de23051e3c6ad69dcfb1798f07de9b5\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/ae/e8/a0/03628c77c2e0aa813f067f6d7708a4579d15abf6f45e8716c5\n",
      "  Building wheel for lmdb (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lmdb: filename=lmdb-0.98-cp36-cp36m-linux_x86_64.whl size=100676 sha256=9b7c35da8ad82e389cdc1106f302187b7b2522ff2488a351bda654350f7f0b67\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/6e/53/3d/5a93174b38712013b3a3b3df15ea2a5144bd11b22edb84a14b\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=2573d6c3f5860775d2d26692762b648576756643e31441b8e46d28e5f8b98d25\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "  Building wheel for demjson (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for demjson: filename=demjson-2.2.4-py3-none-any.whl size=73544 sha256=e4758a0e3271ae1fa0e14359dd98cf3d4d8a8dbd65f2d160c2a7cb8ccaf018f9\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/f6/ce/3a/713fa88864a1d8c48bdd1bacfd18068dd8facc280c4c3c5235\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=3f90f54e44824e39e07185cfe8b9d63d026aabc7b74b29f965e8b5a4a527cf0b\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sklearn fasttext lmdb termcolor demjson sacremoses\n",
      "\u001b[31mERROR: fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\u001b[0m\n",
      "Installing collected packages: sklearn, tqdm, sentencepiece, requests, torch, torchtext, fasttext, lmdb, termcolor, smmap, gitdb, GitPython, editdistance, omegaconf, regex, sacremoses, tokenizers, transformers, torchvision, demjson, mmf\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.42.1\n",
      "    Uninstalling tqdm-4.42.1:\n",
      "      Successfully uninstalled tqdm-4.42.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.22.0\n",
      "    Uninstalling requests-2.22.0:\n",
      "      Successfully uninstalled requests-2.22.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.5.0\n",
      "    Uninstalling torchvision-0.5.0:\n",
      "      Successfully uninstalled torchvision-0.5.0\n",
      "  Running setup.py develop for mmf\n",
      "Successfully installed GitPython-3.1.0 demjson-2.2.4 editdistance-0.5.3 fasttext-0.9.1 gitdb-4.0.5 lmdb-0.98 mmf omegaconf-2.0.1rc4 regex-2020.11.11 requests-2.23.0 sacremoses-0.0.43 sentencepiece-0.1.94 sklearn-0.0 smmap-3.0.4 termcolor-1.1.0 tokenizers-0.9.2 torch-1.6.0 torchtext-0.5.0 torchvision-0.7.0 tqdm-4.51.0 transformers-3.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --editable mmf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WHPkJy-YY5yj",
    "outputId": "d25cca6b-1cad-4226-dad4-d2c43e9070dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hvLH6kgrZQXA"
   },
   "outputs": [],
   "source": [
    "!cp drive/My\\ Drive/MultiModal/Lnmwdnq3YcF7F3YsJncp.zip .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0JzJRqEagvE",
    "outputId": "347ba7d5-c164-43a8-dd96-10ccad6218d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/bin/mmf_convert_hm\", line 33, in <module>\r\n",
      "    sys.exit(load_entry_point('mmf', 'console_scripts', 'mmf_convert_hm')())\r\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/bin/mmf_convert_hm\", line 22, in importlib_load_entry_point\r\n",
      "    for entry_point in distribution(dist_name).entry_points\r\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/importlib_metadata/__init__.py\", line 558, in distribution\r\n",
      "    return Distribution.from_name(distribution_name)\r\n",
      "  File \"/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/importlib_metadata/__init__.py\", line 215, in from_name\r\n",
      "    raise PackageNotFoundError(name)\r\n",
      "importlib_metadata.PackageNotFoundError: No package metadata was found for mmf\r\n"
     ]
    }
   ],
   "source": [
    "#!unzip -P KexZs4tn8hujn1nK Lnmwdnq3YcF7F3YsJncp.zip\n",
    "\n",
    "!mmf_convert_hm --zip_file=Lnmwdnq3YcF7F3YsJncp.zip --password=EWryfbZyNviilcDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XlRtxpiyAFl6",
    "outputId": "a7a286d3-ef46-474e-f4b3-707063d93dfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-25 22:54:30.179452: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "\u001b[32m2020-09-25T22:54:34 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/unimodal/image.yaml\n",
      "\u001b[32m2020-09-25T22:54:34 | mmf.utils.configuration: \u001b[0mOverriding option model to unimodal_image\n",
      "\u001b[32m2020-09-25T22:54:34 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2020-09-25T22:54:34 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2020-09-25T22:54:34 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/unimodal/image.yaml', 'model=unimodal_image', 'dataset=hateful_memes'])\n",
      "\u001b[32m2020-09-25T22:54:34 | mmf_cli.run: \u001b[0mTorch version: 1.6.0+cu101\n",
      "\u001b[32m2020-09-25T22:54:34 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2020-09-25T22:54:34 | mmf_cli.run: \u001b[0mUsing seed 34490205\n",
      "\u001b[32m2020-09-25T22:54:34 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "[ Downloading: https://dl.fbaipublicfiles.com/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz to /root/.cache/torch/mmf/data/datasets/hateful_memes/defaults/extras.tar.gz ]\n",
      "Downloading extras.tar.gz: 100% 211k/211k [00:00<00:00, 1.14MB/s]\n",
      "[ Starting checksum for extras.tar.gz]\n",
      "[ Checksum successful for extras.tar.gz]\n",
      "Unpacking extras.tar.gz\n",
      "\u001b[32m2020-09-25T22:54:35 | torchtext.vocab: \u001b[0mDownloading vectors from http://nlp.stanford.edu/data/glove.6B.zip\n",
      "/root/.cache/torch/mmf/glove.6B.zip: 862MB [06:26, 2.23MB/s]              \n",
      "\u001b[32m2020-09-25T23:01:02 | torchtext.vocab: \u001b[0mExtracting vectors into /root/.cache/torch/mmf\n",
      "\u001b[32m2020-09-25T23:01:27 | torchtext.vocab: \u001b[0mLoading vectors from /root/.cache/torch/mmf/glove.6B.300d.txt\n",
      "100% 399999/400000 [00:35<00:00, 11238.63it/s]\n",
      "\u001b[32m2020-09-25T23:02:08 | torchtext.vocab: \u001b[0mSaving vectors to /root/.cache/torch/mmf/glove.6B.300d.txt.pt\n",
      "\u001b[32m2020-09-25T23:02:10 | torchtext.vocab: \u001b[0mLoading vectors from /root/.cache/torch/mmf/glove.6B.300d.txt.pt\n",
      "\u001b[32m2020-09-25T23:02:13 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n",
      "100% 230M/230M [00:05<00:00, 47.6MB/s]\n",
      "\u001b[32m2020-09-25T23:02:34 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2020-09-25T23:02:34 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2020-09-25T23:02:34 | mmf.trainers.mmf_trainer: \u001b[0m===== Model =====\n",
      "\u001b[32m2020-09-25T23:02:34 | mmf.trainers.mmf_trainer: \u001b[0mUnimodalModal(\n",
      "  (base): UnimodalBase(\n",
      "    (encoder): ResNet152ImageEncoder(\n",
      "      (model): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (4): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (6): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (7): Bottleneck(\n",
      "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (3): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (4): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (5): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (6): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (7): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (8): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (9): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (10): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (11): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (12): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (13): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (14): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (15): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (16): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (17): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (18): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (19): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (20): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (21): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (22): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (23): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (24): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (25): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (26): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (27): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (28): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (29): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (30): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (31): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (32): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (33): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (34): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (35): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): Bottleneck(\n",
      "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (2): Bottleneck(\n",
      "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (classifier): MLPClassifer(\n",
      "    (layers): ModuleList(\n",
      "      (0): Linear(in_features=2048, out_features=768, bias=True)\n",
      "      (1): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Dropout(p=0.5, inplace=False)\n",
      "      (4): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (5): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU()\n",
      "      (7): Dropout(p=0.5, inplace=False)\n",
      "      (8): Linear(in_features=768, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (losses): Losses()\n",
      ")\n",
      "\u001b[32m2020-09-25T23:02:34 | mmf.utils.general: \u001b[0mTotal Parameters: 60312642. Trained Parameters: 60312642\n",
      "\u001b[32m2020-09-25T23:02:34 | mmf.trainers.core.training_loop: \u001b[0mStarting training...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:02:37 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:02:37 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:146: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "\n",
      "\u001b[32m2020-09-25T23:04:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 100/22000, train/total_loss: 0.7029, train/total_loss/avg: 0.7029, train/hateful_memes/cross_entropy: 0.7029, train/hateful_memes/cross_entropy/avg: 0.7029, max mem: 6424.0, experiment: run, epoch: 1, num_updates: 100, iterations: 100, max_updates: 22000, lr: 0., ups: 1.14, time: 01m 28s 303ms, time_since_start: 01m 28s 393ms, eta: 05h 22m 18s 451ms\n",
      "\u001b[32m2020-09-25T23:05:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 200/22000, train/total_loss: 0.7029, train/total_loss/avg: 0.7339, train/hateful_memes/cross_entropy: 0.7029, train/hateful_memes/cross_entropy/avg: 0.7339, max mem: 6424.0, experiment: run, epoch: 1, num_updates: 200, iterations: 200, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 601ms, time_since_start: 03m 01s 995ms, eta: 05h 40m 05s 227ms\n",
      "\u001b[32m2020-09-25T23:07:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 300/22000, train/total_loss: 0.7369, train/total_loss/avg: 0.7349, train/hateful_memes/cross_entropy: 0.7369, train/hateful_memes/cross_entropy/avg: 0.7349, max mem: 6424.0, experiment: run, epoch: 2, num_updates: 300, iterations: 300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 250ms, time_since_start: 04m 37s 246ms, eta: 05h 44m 29s 410ms\n",
      "\u001b[32m2020-09-25T23:08:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 400/22000, train/total_loss: 0.7131, train/total_loss/avg: 0.7295, train/hateful_memes/cross_entropy: 0.7131, train/hateful_memes/cross_entropy/avg: 0.7295, max mem: 6424.0, experiment: run, epoch: 2, num_updates: 400, iterations: 400, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 528ms, time_since_start: 06m 10s 775ms, eta: 05h 36m 42s 238ms\n",
      "\u001b[32m2020-09-25T23:10:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 500/22000, train/total_loss: 0.7220, train/total_loss/avg: 0.7280, train/hateful_memes/cross_entropy: 0.7220, train/hateful_memes/cross_entropy/avg: 0.7280, max mem: 6424.0, experiment: run, epoch: 2, num_updates: 500, iterations: 500, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 150ms, time_since_start: 07m 43s 925ms, eta: 05h 33m 47s 290ms\n",
      "\u001b[32m2020-09-25T23:11:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 600/22000, train/total_loss: 0.7220, train/total_loss/avg: 0.7371, train/hateful_memes/cross_entropy: 0.7220, train/hateful_memes/cross_entropy/avg: 0.7371, max mem: 6424.0, experiment: run, epoch: 3, num_updates: 600, iterations: 600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 576ms, time_since_start: 09m 19s 502ms, eta: 05h 40m 53s 421ms\n",
      "\u001b[32m2020-09-25T23:13:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 700/22000, train/total_loss: 0.7369, train/total_loss/avg: 0.7493, train/hateful_memes/cross_entropy: 0.7369, train/hateful_memes/cross_entropy/avg: 0.7493, max mem: 6424.0, experiment: run, epoch: 3, num_updates: 700, iterations: 700, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 562ms, time_since_start: 10m 53s 065ms, eta: 05h 32m 08s 908ms\n",
      "\u001b[32m2020-09-25T23:15:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 800/22000, train/total_loss: 0.7220, train/total_loss/avg: 0.7440, train/hateful_memes/cross_entropy: 0.7220, train/hateful_memes/cross_entropy/avg: 0.7440, max mem: 6424.0, experiment: run, epoch: 4, num_updates: 800, iterations: 800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 536ms, time_since_start: 12m 28s 602ms, eta: 05h 37m 33s 766ms\n",
      "\u001b[32m2020-09-25T23:16:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 900/22000, train/total_loss: 0.7220, train/total_loss/avg: 0.7199, train/hateful_memes/cross_entropy: 0.7220, train/hateful_memes/cross_entropy/avg: 0.7199, max mem: 6424.0, experiment: run, epoch: 4, num_updates: 900, iterations: 900, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 980ms, time_since_start: 14m 02s 582ms, eta: 05h 30m 29s 814ms\n",
      "\u001b[32m2020-09-25T23:18:11 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:18:11 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:18:11 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-25T23:18:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, train/total_loss: 0.7131, train/total_loss/avg: 0.7081, train/hateful_memes/cross_entropy: 0.7131, train/hateful_memes/cross_entropy/avg: 0.7081, max mem: 6424.0, experiment: run, epoch: 4, num_updates: 1000, iterations: 1000, max_updates: 22000, lr: 0.00001, ups: 1.00, time: 01m 40s 427ms, time_since_start: 15m 43s 009ms, eta: 05h 51m 29s 799ms\n",
      "\u001b[32m2020-09-25T23:18:17 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:18:28 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:18:28 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-25T23:19:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1000/22000, val/total_loss: 0.7274, val/hateful_memes/cross_entropy: 0.7274, val/hateful_memes/accuracy: 0.5080, val/hateful_memes/binary_f1: 0.2264, val/hateful_memes/roc_auc: 0.5084, num_updates: 1000, epoch: 4, iterations: 1000, max_updates: 22000, val_time: 57s 371ms, best_update: 1000, best_iteration: 1000, best_val/hateful_memes/roc_auc: 0.508448\n",
      "\u001b[32m2020-09-25T23:20:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1100/22000, train/total_loss: 0.7131, train/total_loss/avg: 0.7010, train/hateful_memes/cross_entropy: 0.7131, train/hateful_memes/cross_entropy/avg: 0.7010, max mem: 6424.0, experiment: run, epoch: 5, num_updates: 1100, iterations: 1100, max_updates: 22000, lr: 0.00001, ups: 1.03, time: 01m 37s 351ms, time_since_start: 18m 17s 734ms, eta: 05h 39m 06s 559ms\n",
      "\u001b[32m2020-09-25T23:22:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1200/22000, train/total_loss: 0.7067, train/total_loss/avg: 0.6897, train/hateful_memes/cross_entropy: 0.7067, train/hateful_memes/cross_entropy/avg: 0.6897, max mem: 6424.0, experiment: run, epoch: 5, num_updates: 1200, iterations: 1200, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 739ms, time_since_start: 19m 51s 473ms, eta: 05h 24m 57s 729ms\n",
      "\u001b[32m2020-09-25T23:24:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1300/22000, train/total_loss: 0.7067, train/total_loss/avg: 0.6799, train/hateful_memes/cross_entropy: 0.7067, train/hateful_memes/cross_entropy/avg: 0.6799, max mem: 6424.0, experiment: run, epoch: 5, num_updates: 1300, iterations: 1300, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 792ms, time_since_start: 21m 25s 266ms, eta: 05h 23m 35s 153ms\n",
      "\u001b[32m2020-09-25T23:25:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1400/22000, train/total_loss: 0.7029, train/total_loss/avg: 0.6793, train/hateful_memes/cross_entropy: 0.7029, train/hateful_memes/cross_entropy/avg: 0.6793, max mem: 6424.0, experiment: run, epoch: 6, num_updates: 1400, iterations: 1400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 926ms, time_since_start: 23m 01s 193ms, eta: 05h 29m 20s 958ms\n",
      "\u001b[32m2020-09-25T23:27:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1500/22000, train/total_loss: 0.7029, train/total_loss/avg: 0.6700, train/hateful_memes/cross_entropy: 0.7029, train/hateful_memes/cross_entropy/avg: 0.6700, max mem: 6424.0, experiment: run, epoch: 6, num_updates: 1500, iterations: 1500, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 859ms, time_since_start: 24m 35s 053ms, eta: 05h 20m 41s 245ms\n",
      "\u001b[32m2020-09-25T23:28:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1600/22000, train/total_loss: 0.6988, train/total_loss/avg: 0.6718, train/hateful_memes/cross_entropy: 0.6988, train/hateful_memes/cross_entropy/avg: 0.6718, max mem: 6424.0, experiment: run, epoch: 7, num_updates: 1600, iterations: 1600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 432ms, time_since_start: 26m 10s 485ms, eta: 05h 24m 28s 153ms\n",
      "\u001b[32m2020-09-25T23:30:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1700/22000, train/total_loss: 0.6988, train/total_loss/avg: 0.6590, train/hateful_memes/cross_entropy: 0.6988, train/hateful_memes/cross_entropy/avg: 0.6590, max mem: 6424.0, experiment: run, epoch: 7, num_updates: 1700, iterations: 1700, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 724ms, time_since_start: 27m 44s 210ms, eta: 05h 17m 06s 120ms\n",
      "\u001b[32m2020-09-25T23:31:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1800/22000, train/total_loss: 0.6718, train/total_loss/avg: 0.6498, train/hateful_memes/cross_entropy: 0.6718, train/hateful_memes/cross_entropy/avg: 0.6498, max mem: 6424.0, experiment: run, epoch: 7, num_updates: 1800, iterations: 1800, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 370ms, time_since_start: 29m 17s 580ms, eta: 05h 14m 20s 807ms\n",
      "\u001b[32m2020-09-25T23:33:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 1900/22000, train/total_loss: 0.6718, train/total_loss/avg: 0.6470, train/hateful_memes/cross_entropy: 0.6718, train/hateful_memes/cross_entropy/avg: 0.6470, max mem: 6424.0, experiment: run, epoch: 8, num_updates: 1900, iterations: 1900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 304ms, time_since_start: 30m 52s 885ms, eta: 05h 19m 16s 208ms\n",
      "\u001b[32m2020-09-25T23:35:01 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:35:01 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:35:01 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-25T23:35:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, train/total_loss: 0.6302, train/total_loss/avg: 0.6457, train/hateful_memes/cross_entropy: 0.6302, train/hateful_memes/cross_entropy/avg: 0.6457, max mem: 6424.0, experiment: run, epoch: 8, num_updates: 2000, iterations: 2000, max_updates: 22000, lr: 0.00001, ups: 1.00, time: 01m 40s 296ms, time_since_start: 32m 33s 181ms, eta: 05h 34m 19s 358ms\n",
      "\u001b[32m2020-09-25T23:35:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:35:16 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:35:16 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-25T23:36:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2000/22000, val/total_loss: 0.8769, val/hateful_memes/cross_entropy: 0.8769, val/hateful_memes/accuracy: 0.5100, val/hateful_memes/binary_f1: 0.3603, val/hateful_memes/roc_auc: 0.5157, num_updates: 2000, epoch: 8, iterations: 2000, max_updates: 22000, val_time: 54s 967ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.515696\n",
      "\u001b[32m2020-09-25T23:37:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2100/22000, train/total_loss: 0.6225, train/total_loss/avg: 0.6421, train/hateful_memes/cross_entropy: 0.6225, train/hateful_memes/cross_entropy/avg: 0.6421, max mem: 6424.0, experiment: run, epoch: 8, num_updates: 2100, iterations: 2100, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 202ms, time_since_start: 35m 03s 352ms, eta: 05h 15m 45s 265ms\n",
      "\u001b[32m2020-09-25T23:39:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2200/22000, train/total_loss: 0.6020, train/total_loss/avg: 0.6271, train/hateful_memes/cross_entropy: 0.6020, train/hateful_memes/cross_entropy/avg: 0.6271, max mem: 6424.0, experiment: run, epoch: 9, num_updates: 2200, iterations: 2200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 177ms, time_since_start: 36m 38s 530ms, eta: 05h 14m 05s 247ms\n",
      "\u001b[32m2020-09-25T23:40:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2300/22000, train/total_loss: 0.5961, train/total_loss/avg: 0.6165, train/hateful_memes/cross_entropy: 0.5961, train/hateful_memes/cross_entropy/avg: 0.6165, max mem: 6424.0, experiment: run, epoch: 9, num_updates: 2300, iterations: 2300, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 113ms, time_since_start: 38m 11s 644ms, eta: 05h 05m 43s 404ms\n",
      "\u001b[32m2020-09-25T23:42:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2400/22000, train/total_loss: 0.5693, train/total_loss/avg: 0.5989, train/hateful_memes/cross_entropy: 0.5693, train/hateful_memes/cross_entropy/avg: 0.5989, max mem: 6424.0, experiment: run, epoch: 10, num_updates: 2400, iterations: 2400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 444ms, time_since_start: 39m 47s 088ms, eta: 05h 11m 47s 098ms\n",
      "\u001b[32m2020-09-25T23:43:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2500/22000, train/total_loss: 0.5654, train/total_loss/avg: 0.5849, train/hateful_memes/cross_entropy: 0.5654, train/hateful_memes/cross_entropy/avg: 0.5849, max mem: 6424.0, experiment: run, epoch: 10, num_updates: 2500, iterations: 2500, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 957ms, time_since_start: 41m 21s 046ms, eta: 05h 05m 21s 689ms\n",
      "\u001b[32m2020-09-25T23:45:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2600/22000, train/total_loss: 0.5623, train/total_loss/avg: 0.5674, train/hateful_memes/cross_entropy: 0.5623, train/hateful_memes/cross_entropy/avg: 0.5674, max mem: 6424.0, experiment: run, epoch: 10, num_updates: 2600, iterations: 2600, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 893ms, time_since_start: 42m 54s 939ms, eta: 05h 03m 35s 314ms\n",
      "\u001b[32m2020-09-25T23:47:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2700/22000, train/total_loss: 0.5400, train/total_loss/avg: 0.5523, train/hateful_memes/cross_entropy: 0.5400, train/hateful_memes/cross_entropy/avg: 0.5523, max mem: 6424.0, experiment: run, epoch: 11, num_updates: 2700, iterations: 2700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 621ms, time_since_start: 44m 30s 560ms, eta: 05h 07m 34s 887ms\n",
      "\u001b[32m2020-09-25T23:48:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2800/22000, train/total_loss: 0.5269, train/total_loss/avg: 0.5380, train/hateful_memes/cross_entropy: 0.5269, train/hateful_memes/cross_entropy/avg: 0.5380, max mem: 6424.0, experiment: run, epoch: 11, num_updates: 2800, iterations: 2800, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 927ms, time_since_start: 46m 04s 488ms, eta: 05h 34s 069ms\n",
      "\u001b[32m2020-09-25T23:50:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 2900/22000, train/total_loss: 0.4932, train/total_loss/avg: 0.5226, train/hateful_memes/cross_entropy: 0.4932, train/hateful_memes/cross_entropy/avg: 0.5226, max mem: 6424.0, experiment: run, epoch: 11, num_updates: 2900, iterations: 2900, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 616ms, time_since_start: 47m 38s 104ms, eta: 04h 58m 688ms\n",
      "\u001b[32m2020-09-25T23:51:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:51:48 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:51:48 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-25T23:51:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, train/total_loss: 0.4536, train/total_loss/avg: 0.5072, train/hateful_memes/cross_entropy: 0.4536, train/hateful_memes/cross_entropy/avg: 0.5072, max mem: 6424.0, experiment: run, epoch: 12, num_updates: 3000, iterations: 3000, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 672ms, time_since_start: 49m 19s 777ms, eta: 05h 21m 57s 821ms\n",
      "\u001b[32m2020-09-25T23:51:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:52:03 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-25T23:52:03 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-25T23:52:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3000/22000, val/total_loss: 1.4028, val/hateful_memes/cross_entropy: 1.4028, val/hateful_memes/accuracy: 0.4860, val/hateful_memes/binary_f1: 0.3591, val/hateful_memes/roc_auc: 0.4872, num_updates: 3000, epoch: 12, iterations: 3000, max_updates: 22000, val_time: 38s 311ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.515696\n",
      "\u001b[32m2020-09-25T23:54:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3100/22000, train/total_loss: 0.3828, train/total_loss/avg: 0.4952, train/hateful_memes/cross_entropy: 0.3828, train/hateful_memes/cross_entropy/avg: 0.4952, max mem: 6424.0, experiment: run, epoch: 12, num_updates: 3100, iterations: 3100, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 696ms, time_since_start: 51m 32s 785ms, eta: 04h 58m 17s 559ms\n",
      "\u001b[32m2020-09-25T23:55:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3200/22000, train/total_loss: 0.3122, train/total_loss/avg: 0.4806, train/hateful_memes/cross_entropy: 0.3122, train/hateful_memes/cross_entropy/avg: 0.4806, max mem: 6424.0, experiment: run, epoch: 13, num_updates: 3200, iterations: 3200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 193ms, time_since_start: 53m 07s 979ms, eta: 04h 58m 16s 466ms\n",
      "\u001b[32m2020-09-25T23:57:16 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3300/22000, train/total_loss: 0.2496, train/total_loss/avg: 0.4671, train/hateful_memes/cross_entropy: 0.2496, train/hateful_memes/cross_entropy/avg: 0.4671, max mem: 6424.0, experiment: run, epoch: 13, num_updates: 3300, iterations: 3300, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 465ms, time_since_start: 54m 41s 445ms, eta: 04h 51m 18s 129ms\n",
      "\u001b[32m2020-09-25T23:58:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3400/22000, train/total_loss: 0.1942, train/total_loss/avg: 0.4545, train/hateful_memes/cross_entropy: 0.1942, train/hateful_memes/cross_entropy/avg: 0.4545, max mem: 6424.0, experiment: run, epoch: 13, num_updates: 3400, iterations: 3400, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 102ms, time_since_start: 56m 14s 548ms, eta: 04h 48m 37s 059ms\n",
      "\u001b[32m2020-09-26T00:00:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3500/22000, train/total_loss: 0.1603, train/total_loss/avg: 0.4424, train/hateful_memes/cross_entropy: 0.1603, train/hateful_memes/cross_entropy/avg: 0.4424, max mem: 6424.0, experiment: run, epoch: 14, num_updates: 3500, iterations: 3500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 028ms, time_since_start: 57m 49s 577ms, eta: 04h 53m 365ms\n",
      "\u001b[32m2020-09-26T00:01:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3600/22000, train/total_loss: 0.1515, train/total_loss/avg: 0.4310, train/hateful_memes/cross_entropy: 0.1515, train/hateful_memes/cross_entropy/avg: 0.4310, max mem: 6424.0, experiment: run, epoch: 14, num_updates: 3600, iterations: 3600, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 331ms, time_since_start: 59m 22s 908ms, eta: 04h 46m 12s 937ms\n",
      "\u001b[32m2020-09-26T00:03:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3700/22000, train/total_loss: 0.1332, train/total_loss/avg: 0.4198, train/hateful_memes/cross_entropy: 0.1332, train/hateful_memes/cross_entropy/avg: 0.4198, max mem: 6424.0, experiment: run, epoch: 14, num_updates: 3700, iterations: 3700, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 407ms, time_since_start: 01h 56s 316ms, eta: 04h 44m 53s 631ms\n",
      "\u001b[32m2020-09-26T00:05:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3800/22000, train/total_loss: 0.1292, train/total_loss/avg: 0.4090, train/hateful_memes/cross_entropy: 0.1292, train/hateful_memes/cross_entropy/avg: 0.4090, max mem: 6424.0, experiment: run, epoch: 15, num_updates: 3800, iterations: 3800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 438ms, time_since_start: 01h 02m 31s 755ms, eta: 04h 49m 29s 885ms\n",
      "\u001b[32m2020-09-26T00:06:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 3900/22000, train/total_loss: 0.0926, train/total_loss/avg: 0.3989, train/hateful_memes/cross_entropy: 0.0926, train/hateful_memes/cross_entropy/avg: 0.3989, max mem: 6424.0, experiment: run, epoch: 15, num_updates: 3900, iterations: 3900, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 524ms, time_since_start: 01h 04m 05s 279ms, eta: 04h 42m 07s 887ms\n",
      "\u001b[32m2020-09-26T00:08:15 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:08:15 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:08:15 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T00:08:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, train/total_loss: 0.0607, train/total_loss/avg: 0.3896, train/hateful_memes/cross_entropy: 0.0607, train/hateful_memes/cross_entropy/avg: 0.3896, max mem: 6424.0, experiment: run, epoch: 16, num_updates: 4000, iterations: 4000, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 909ms, time_since_start: 01h 05m 47s 188ms, eta: 05h 05m 43s 678ms\n",
      "\u001b[32m2020-09-26T00:08:22 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:08:30 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:08:30 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T00:08:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4000/22000, val/total_loss: 1.6092, val/hateful_memes/cross_entropy: 1.6092, val/hateful_memes/accuracy: 0.5120, val/hateful_memes/binary_f1: 0.4554, val/hateful_memes/roc_auc: 0.4866, num_updates: 4000, epoch: 16, iterations: 4000, max_updates: 22000, val_time: 34s 735ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.515696\n",
      "\u001b[32m2020-09-26T00:10:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4100/22000, train/total_loss: 0.0377, train/total_loss/avg: 0.3804, train/hateful_memes/cross_entropy: 0.0377, train/hateful_memes/cross_entropy/avg: 0.3804, max mem: 6424.0, experiment: run, epoch: 16, num_updates: 4100, iterations: 4100, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 575ms, time_since_start: 01h 07m 56s 716ms, eta: 04h 42m 08s 934ms\n",
      "\u001b[32m2020-09-26T00:12:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4200/22000, train/total_loss: 0.0377, train/total_loss/avg: 0.3729, train/hateful_memes/cross_entropy: 0.0377, train/hateful_memes/cross_entropy/avg: 0.3729, max mem: 6424.0, experiment: run, epoch: 16, num_updates: 4200, iterations: 4200, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 284ms, time_since_start: 01h 09m 30s 001ms, eta: 04h 36m 44s 701ms\n",
      "\u001b[32m2020-09-26T00:13:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4300/22000, train/total_loss: 0.0377, train/total_loss/avg: 0.3652, train/hateful_memes/cross_entropy: 0.0377, train/hateful_memes/cross_entropy/avg: 0.3652, max mem: 6424.0, experiment: run, epoch: 17, num_updates: 4300, iterations: 4300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 342ms, time_since_start: 01h 11m 05s 343ms, eta: 04h 41m 15s 621ms\n",
      "\u001b[32m2020-09-26T00:15:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4400/22000, train/total_loss: 0.0377, train/total_loss/avg: 0.3578, train/hateful_memes/cross_entropy: 0.0377, train/hateful_memes/cross_entropy/avg: 0.3578, max mem: 6424.0, experiment: run, epoch: 17, num_updates: 4400, iterations: 4400, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 196ms, time_since_start: 01h 12m 38s 540ms, eta: 04h 33m 22s 617ms\n",
      "\u001b[32m2020-09-26T00:16:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4500/22000, train/total_loss: 0.0359, train/total_loss/avg: 0.3504, train/hateful_memes/cross_entropy: 0.0359, train/hateful_memes/cross_entropy/avg: 0.3504, max mem: 6424.0, experiment: run, epoch: 17, num_updates: 4500, iterations: 4500, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 371ms, time_since_start: 01h 14m 11s 912ms, eta: 04h 32m 20s 099ms\n",
      "\u001b[32m2020-09-26T00:18:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4600/22000, train/total_loss: 0.0322, train/total_loss/avg: 0.3431, train/hateful_memes/cross_entropy: 0.0322, train/hateful_memes/cross_entropy/avg: 0.3431, max mem: 6424.0, experiment: run, epoch: 18, num_updates: 4600, iterations: 4600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 248ms, time_since_start: 01h 15m 47s 160ms, eta: 04h 36m 13s 197ms\n",
      "\u001b[32m2020-09-26T00:19:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4700/22000, train/total_loss: 0.0303, train/total_loss/avg: 0.3361, train/hateful_memes/cross_entropy: 0.0303, train/hateful_memes/cross_entropy/avg: 0.3361, max mem: 6424.0, experiment: run, epoch: 18, num_updates: 4700, iterations: 4700, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 248ms, time_since_start: 01h 17m 20s 409ms, eta: 04h 28m 51s 991ms\n",
      "\u001b[32m2020-09-26T00:21:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4800/22000, train/total_loss: 0.0294, train/total_loss/avg: 0.3293, train/hateful_memes/cross_entropy: 0.0294, train/hateful_memes/cross_entropy/avg: 0.3293, max mem: 6424.0, experiment: run, epoch: 19, num_updates: 4800, iterations: 4800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 099ms, time_since_start: 01h 18m 55s 509ms, eta: 04h 32m 37s 170ms\n",
      "\u001b[32m2020-09-26T00:23:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 4900/22000, train/total_loss: 0.0253, train/total_loss/avg: 0.3230, train/hateful_memes/cross_entropy: 0.0253, train/hateful_memes/cross_entropy/avg: 0.3230, max mem: 6424.0, experiment: run, epoch: 19, num_updates: 4900, iterations: 4900, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 543ms, time_since_start: 01h 20m 29s 052ms, eta: 04h 26m 35s 947ms\n",
      "\u001b[32m2020-09-26T00:24:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:24:37 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:24:37 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T00:24:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, train/total_loss: 0.0253, train/total_loss/avg: 0.3168, train/hateful_memes/cross_entropy: 0.0253, train/hateful_memes/cross_entropy/avg: 0.3168, max mem: 6424.0, experiment: run, epoch: 19, num_updates: 5000, iterations: 5000, max_updates: 22000, lr: 0.00001, ups: 1.00, time: 01m 40s 095ms, time_since_start: 01h 22m 09s 147ms, eta: 04h 43m 36s 189ms\n",
      "\u001b[32m2020-09-26T00:24:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:24:52 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:24:52 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T00:25:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5000/22000, val/total_loss: 1.8285, val/hateful_memes/cross_entropy: 1.8285, val/hateful_memes/accuracy: 0.5260, val/hateful_memes/binary_f1: 0.4030, val/hateful_memes/roc_auc: 0.5055, num_updates: 5000, epoch: 19, iterations: 5000, max_updates: 22000, val_time: 35s 247ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.515696\n",
      "\u001b[32m2020-09-26T00:26:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5100/22000, train/total_loss: 0.0216, train/total_loss/avg: 0.3107, train/hateful_memes/cross_entropy: 0.0216, train/hateful_memes/cross_entropy/avg: 0.3107, max mem: 6424.0, experiment: run, epoch: 20, num_updates: 5100, iterations: 5100, max_updates: 22000, lr: 0.00001, ups: 1.01, time: 01m 39s 834ms, time_since_start: 01h 24m 24s 293ms, eta: 04h 41m 12s 042ms\n",
      "\u001b[32m2020-09-26T00:28:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5200/22000, train/total_loss: 0.0170, train/total_loss/avg: 0.3049, train/hateful_memes/cross_entropy: 0.0170, train/hateful_memes/cross_entropy/avg: 0.3049, max mem: 6424.0, experiment: run, epoch: 20, num_updates: 5200, iterations: 5200, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 384ms, time_since_start: 01h 25m 57s 677ms, eta: 04h 21m 28s 631ms\n",
      "\u001b[32m2020-09-26T00:30:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5300/22000, train/total_loss: 0.0154, train/total_loss/avg: 0.2993, train/hateful_memes/cross_entropy: 0.0154, train/hateful_memes/cross_entropy/avg: 0.2993, max mem: 6424.0, experiment: run, epoch: 20, num_updates: 5300, iterations: 5300, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 317ms, time_since_start: 01h 27m 30s 995ms, eta: 04h 19m 44s 028ms\n",
      "\u001b[32m2020-09-26T00:31:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5400/22000, train/total_loss: 0.0154, train/total_loss/avg: 0.2946, train/hateful_memes/cross_entropy: 0.0154, train/hateful_memes/cross_entropy/avg: 0.2946, max mem: 6424.0, experiment: run, epoch: 21, num_updates: 5400, iterations: 5400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 382ms, time_since_start: 01h 29m 06s 377ms, eta: 04h 23m 53s 477ms\n",
      "\u001b[32m2020-09-26T00:33:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5500/22000, train/total_loss: 0.0140, train/total_loss/avg: 0.2895, train/hateful_memes/cross_entropy: 0.0140, train/hateful_memes/cross_entropy/avg: 0.2895, max mem: 6424.0, experiment: run, epoch: 21, num_updates: 5500, iterations: 5500, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 174ms, time_since_start: 01h 30m 39s 552ms, eta: 04h 16m 13s 757ms\n",
      "\u001b[32m2020-09-26T00:34:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5600/22000, train/total_loss: 0.0136, train/total_loss/avg: 0.2846, train/hateful_memes/cross_entropy: 0.0136, train/hateful_memes/cross_entropy/avg: 0.2846, max mem: 6424.0, experiment: run, epoch: 22, num_updates: 5600, iterations: 5600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 229ms, time_since_start: 01h 32m 14s 781ms, eta: 04h 20m 17s 620ms\n",
      "\u001b[32m2020-09-26T00:36:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5700/22000, train/total_loss: 0.0135, train/total_loss/avg: 0.2797, train/hateful_memes/cross_entropy: 0.0135, train/hateful_memes/cross_entropy/avg: 0.2797, max mem: 6424.0, experiment: run, epoch: 22, num_updates: 5700, iterations: 5700, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 339ms, time_since_start: 01h 33m 48s 121ms, eta: 04h 13m 34s 381ms\n",
      "\u001b[32m2020-09-26T00:37:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5800/22000, train/total_loss: 0.0135, train/total_loss/avg: 0.2750, train/hateful_memes/cross_entropy: 0.0135, train/hateful_memes/cross_entropy/avg: 0.2750, max mem: 6424.0, experiment: run, epoch: 22, num_updates: 5800, iterations: 5800, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 514ms, time_since_start: 01h 35m 21s 636ms, eta: 04h 12m 29s 411ms\n",
      "\u001b[32m2020-09-26T00:39:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 5900/22000, train/total_loss: 0.0130, train/total_loss/avg: 0.2705, train/hateful_memes/cross_entropy: 0.0130, train/hateful_memes/cross_entropy/avg: 0.2705, max mem: 6424.0, experiment: run, epoch: 23, num_updates: 5900, iterations: 5900, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 906ms, time_since_start: 01h 36m 56s 542ms, eta: 04h 14m 39s 985ms\n",
      "\u001b[32m2020-09-26T00:41:04 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:41:04 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:41:04 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T00:41:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, train/total_loss: 0.0130, train/total_loss/avg: 0.2662, train/hateful_memes/cross_entropy: 0.0130, train/hateful_memes/cross_entropy/avg: 0.2662, max mem: 6424.0, experiment: run, epoch: 23, num_updates: 6000, iterations: 6000, max_updates: 22000, lr: 0.00001, ups: 1.01, time: 01m 39s 843ms, time_since_start: 01h 38m 36s 386ms, eta: 04h 26m 15s 002ms\n",
      "\u001b[32m2020-09-26T00:41:12 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:41:20 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:41:20 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T00:41:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6000/22000, val/total_loss: 2.0061, val/hateful_memes/cross_entropy: 2.0061, val/hateful_memes/accuracy: 0.5080, val/hateful_memes/binary_f1: 0.3819, val/hateful_memes/roc_auc: 0.4958, num_updates: 6000, epoch: 23, iterations: 6000, max_updates: 22000, val_time: 34s 647ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.515696\n",
      "\u001b[32m2020-09-26T00:43:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6100/22000, train/total_loss: 0.0130, train/total_loss/avg: 0.2623, train/hateful_memes/cross_entropy: 0.0130, train/hateful_memes/cross_entropy/avg: 0.2623, max mem: 6424.0, experiment: run, epoch: 23, num_updates: 6100, iterations: 6100, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 055ms, time_since_start: 01h 40m 47s 190ms, eta: 04h 11m 53s 857ms\n",
      "\u001b[32m2020-09-26T00:44:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6200/22000, train/total_loss: 0.0130, train/total_loss/avg: 0.2585, train/hateful_memes/cross_entropy: 0.0130, train/hateful_memes/cross_entropy/avg: 0.2585, max mem: 6424.0, experiment: run, epoch: 24, num_updates: 6200, iterations: 6200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 416ms, time_since_start: 01h 42m 22s 607ms, eta: 04h 11m 15s 843ms\n",
      "\u001b[32m2020-09-26T00:46:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6300/22000, train/total_loss: 0.0130, train/total_loss/avg: 0.2544, train/hateful_memes/cross_entropy: 0.0130, train/hateful_memes/cross_entropy/avg: 0.2544, max mem: 6424.0, experiment: run, epoch: 24, num_updates: 6300, iterations: 6300, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 347ms, time_since_start: 01h 43m 55s 955ms, eta: 04h 04m 15s 589ms\n",
      "\u001b[32m2020-09-26T00:48:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6400/22000, train/total_loss: 0.0129, train/total_loss/avg: 0.2505, train/hateful_memes/cross_entropy: 0.0129, train/hateful_memes/cross_entropy/avg: 0.2505, max mem: 6424.0, experiment: run, epoch: 25, num_updates: 6400, iterations: 6400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 358ms, time_since_start: 01h 45m 31s 314ms, eta: 04h 07m 56s 008ms\n",
      "\u001b[32m2020-09-26T00:49:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6500/22000, train/total_loss: 0.0088, train/total_loss/avg: 0.2467, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.2467, max mem: 6424.0, experiment: run, epoch: 25, num_updates: 6500, iterations: 6500, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 907ms, time_since_start: 01h 47m 05s 221ms, eta: 04h 02m 35s 611ms\n",
      "\u001b[32m2020-09-26T00:51:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6600/22000, train/total_loss: 0.0085, train/total_loss/avg: 0.2430, train/hateful_memes/cross_entropy: 0.0085, train/hateful_memes/cross_entropy/avg: 0.2430, max mem: 6424.0, experiment: run, epoch: 25, num_updates: 6600, iterations: 6600, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 548ms, time_since_start: 01h 48m 38s 769ms, eta: 04h 06s 404ms\n",
      "\u001b[32m2020-09-26T00:52:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6700/22000, train/total_loss: 0.0085, train/total_loss/avg: 0.2396, train/hateful_memes/cross_entropy: 0.0085, train/hateful_memes/cross_entropy/avg: 0.2396, max mem: 6424.0, experiment: run, epoch: 26, num_updates: 6700, iterations: 6700, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 121ms, time_since_start: 01h 50m 14s 891ms, eta: 04h 05m 06s 650ms\n",
      "\u001b[32m2020-09-26T00:54:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6800/22000, train/total_loss: 0.0088, train/total_loss/avg: 0.2367, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.2367, max mem: 6424.0, experiment: run, epoch: 26, num_updates: 6800, iterations: 6800, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 438ms, time_since_start: 01h 51m 48s 329ms, eta: 03h 56m 42s 587ms\n",
      "\u001b[32m2020-09-26T00:55:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 6900/22000, train/total_loss: 0.0088, train/total_loss/avg: 0.2338, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.2338, max mem: 6424.0, experiment: run, epoch: 26, num_updates: 6900, iterations: 6900, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 803ms, time_since_start: 01h 53m 22s 132ms, eta: 03h 56m 04s 301ms\n",
      "\u001b[32m2020-09-26T00:57:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:57:33 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T00:57:33 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T00:57:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, train/total_loss: 0.0088, train/total_loss/avg: 0.2307, train/hateful_memes/cross_entropy: 0.0088, train/hateful_memes/cross_entropy/avg: 0.2307, max mem: 6424.0, experiment: run, epoch: 27, num_updates: 7000, iterations: 7000, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 078ms, time_since_start: 01h 55m 04s 211ms, eta: 04h 15m 11s 820ms\n",
      "\u001b[32m2020-09-26T00:57:38 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T00:57:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7000/22000, val/total_loss: 2.2425, val/hateful_memes/cross_entropy: 2.2425, val/hateful_memes/accuracy: 0.5100, val/hateful_memes/binary_f1: 0.3570, val/hateful_memes/roc_auc: 0.4979, num_updates: 7000, epoch: 27, iterations: 7000, max_updates: 22000, val_time: 09s 154ms, best_update: 2000, best_iteration: 2000, best_val/hateful_memes/roc_auc: 0.515696\n",
      "\u001b[32m2020-09-26T00:59:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7100/22000, train/total_loss: 0.0103, train/total_loss/avg: 0.2284, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2284, max mem: 6424.0, experiment: run, epoch: 27, num_updates: 7100, iterations: 7100, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 517ms, time_since_start: 01h 56m 47s 884ms, eta: 03h 54m 43s 104ms\n",
      "\u001b[32m2020-09-26T01:00:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7200/22000, train/total_loss: 0.0103, train/total_loss/avg: 0.2252, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2252, max mem: 6424.0, experiment: run, epoch: 28, num_updates: 7200, iterations: 7200, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 233ms, time_since_start: 01h 58m 24s 117ms, eta: 03h 57m 22s 506ms\n",
      "\u001b[32m2020-09-26T01:02:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7300/22000, train/total_loss: 0.0130, train/total_loss/avg: 0.2223, train/hateful_memes/cross_entropy: 0.0130, train/hateful_memes/cross_entropy/avg: 0.2223, max mem: 6424.0, experiment: run, epoch: 28, num_updates: 7300, iterations: 7300, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 645ms, time_since_start: 01h 59m 57s 763ms, eta: 03h 49m 25s 859ms\n",
      "\u001b[32m2020-09-26T01:04:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7400/22000, train/total_loss: 0.0130, train/total_loss/avg: 0.2204, train/hateful_memes/cross_entropy: 0.0130, train/hateful_memes/cross_entropy/avg: 0.2204, max mem: 6424.0, experiment: run, epoch: 28, num_updates: 7400, iterations: 7400, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 038ms, time_since_start: 02h 01m 31s 801ms, eta: 03h 48m 49s 675ms\n",
      "\u001b[32m2020-09-26T01:05:41 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7500/22000, train/total_loss: 0.0103, train/total_loss/avg: 0.2175, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2175, max mem: 6424.0, experiment: run, epoch: 29, num_updates: 7500, iterations: 7500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 289ms, time_since_start: 02h 03m 07s 091ms, eta: 03h 50m 17s 047ms\n",
      "\u001b[32m2020-09-26T01:07:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7600/22000, train/total_loss: 0.0103, train/total_loss/avg: 0.2148, train/hateful_memes/cross_entropy: 0.0103, train/hateful_memes/cross_entropy/avg: 0.2148, max mem: 6424.0, experiment: run, epoch: 29, num_updates: 7600, iterations: 7600, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 283ms, time_since_start: 02h 04m 40s 375ms, eta: 03h 43m 52s 839ms\n",
      "\u001b[32m2020-09-26T01:08:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7700/22000, train/total_loss: 0.0133, train/total_loss/avg: 0.2128, train/hateful_memes/cross_entropy: 0.0133, train/hateful_memes/cross_entropy/avg: 0.2128, max mem: 6424.0, experiment: run, epoch: 29, num_updates: 7700, iterations: 7700, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 881ms, time_since_start: 02h 06m 14s 256ms, eta: 03h 43m 45s 038ms\n",
      "\u001b[32m2020-09-26T01:10:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7800/22000, train/total_loss: 0.0156, train/total_loss/avg: 0.2103, train/hateful_memes/cross_entropy: 0.0156, train/hateful_memes/cross_entropy/avg: 0.2103, max mem: 6424.0, experiment: run, epoch: 30, num_updates: 7800, iterations: 7800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 789ms, time_since_start: 02h 07m 50s 046ms, eta: 03h 46m 42s 076ms\n",
      "\u001b[32m2020-09-26T01:11:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 7900/22000, train/total_loss: 0.0164, train/total_loss/avg: 0.2078, train/hateful_memes/cross_entropy: 0.0164, train/hateful_memes/cross_entropy/avg: 0.2078, max mem: 6424.0, experiment: run, epoch: 30, num_updates: 7900, iterations: 7900, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 440ms, time_since_start: 02h 09m 23s 487ms, eta: 03h 39m 35s 171ms\n",
      "\u001b[32m2020-09-26T01:13:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:13:33 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:13:33 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T01:13:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, train/total_loss: 0.0164, train/total_loss/avg: 0.2053, train/hateful_memes/cross_entropy: 0.0164, train/hateful_memes/cross_entropy/avg: 0.2053, max mem: 6424.0, experiment: run, epoch: 31, num_updates: 8000, iterations: 8000, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 732ms, time_since_start: 02h 11m 05s 219ms, eta: 03h 57m 22s 534ms\n",
      "\u001b[32m2020-09-26T01:13:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:13:49 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:13:49 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T01:14:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8000/22000, val/total_loss: 2.1564, val/hateful_memes/cross_entropy: 2.1564, val/hateful_memes/accuracy: 0.5440, val/hateful_memes/binary_f1: 0.4000, val/hateful_memes/roc_auc: 0.5228, num_updates: 8000, epoch: 31, iterations: 8000, max_updates: 22000, val_time: 54s 374ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T01:16:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8100/22000, train/total_loss: 0.0164, train/total_loss/avg: 0.2030, train/hateful_memes/cross_entropy: 0.0164, train/hateful_memes/cross_entropy/avg: 0.2030, max mem: 6424.0, experiment: run, epoch: 31, num_updates: 8100, iterations: 8100, max_updates: 22000, lr: 0.00001, ups: 1.04, time: 01m 36s 005ms, time_since_start: 02h 13m 35s 601ms, eta: 03h 42m 24s 793ms\n",
      "\u001b[32m2020-09-26T01:17:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8200/22000, train/total_loss: 0.0156, train/total_loss/avg: 0.2006, train/hateful_memes/cross_entropy: 0.0156, train/hateful_memes/cross_entropy/avg: 0.2006, max mem: 6424.0, experiment: run, epoch: 31, num_updates: 8200, iterations: 8200, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 367ms, time_since_start: 02h 15m 08s 969ms, eta: 03h 34m 44s 685ms\n",
      "\u001b[32m2020-09-26T01:19:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8300/22000, train/total_loss: 0.0164, train/total_loss/avg: 0.1988, train/hateful_memes/cross_entropy: 0.0164, train/hateful_memes/cross_entropy/avg: 0.1988, max mem: 6424.0, experiment: run, epoch: 32, num_updates: 8300, iterations: 8300, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 494ms, time_since_start: 02h 16m 44s 463ms, eta: 03h 38m 02s 738ms\n",
      "\u001b[32m2020-09-26T01:20:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8400/22000, train/total_loss: 0.0164, train/total_loss/avg: 0.1964, train/hateful_memes/cross_entropy: 0.0164, train/hateful_memes/cross_entropy/avg: 0.1964, max mem: 6424.0, experiment: run, epoch: 32, num_updates: 8400, iterations: 8400, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 747ms, time_since_start: 02h 18m 18s 211ms, eta: 03h 32m 29s 648ms\n",
      "\u001b[32m2020-09-26T01:22:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8500/22000, train/total_loss: 0.0164, train/total_loss/avg: 0.1942, train/hateful_memes/cross_entropy: 0.0164, train/hateful_memes/cross_entropy/avg: 0.1942, max mem: 6424.0, experiment: run, epoch: 32, num_updates: 8500, iterations: 8500, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 417ms, time_since_start: 02h 19m 51s 628ms, eta: 03h 30m 11s 328ms\n",
      "\u001b[32m2020-09-26T01:24:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8600/22000, train/total_loss: 0.0164, train/total_loss/avg: 0.1919, train/hateful_memes/cross_entropy: 0.0164, train/hateful_memes/cross_entropy/avg: 0.1919, max mem: 6424.0, experiment: run, epoch: 33, num_updates: 8600, iterations: 8600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 582ms, time_since_start: 02h 21m 27s 211ms, eta: 03h 33m 28s 120ms\n",
      "\u001b[32m2020-09-26T01:25:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8700/22000, train/total_loss: 0.0164, train/total_loss/avg: 0.1897, train/hateful_memes/cross_entropy: 0.0164, train/hateful_memes/cross_entropy/avg: 0.1897, max mem: 6424.0, experiment: run, epoch: 33, num_updates: 8700, iterations: 8700, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 216ms, time_since_start: 02h 23m 428ms, eta: 03h 26m 37s 856ms\n",
      "\u001b[32m2020-09-26T01:27:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8800/22000, train/total_loss: 0.0156, train/total_loss/avg: 0.1876, train/hateful_memes/cross_entropy: 0.0156, train/hateful_memes/cross_entropy/avg: 0.1876, max mem: 6424.0, experiment: run, epoch: 34, num_updates: 8800, iterations: 8800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 385ms, time_since_start: 02h 24m 35s 813ms, eta: 03h 29m 50s 862ms\n",
      "\u001b[32m2020-09-26T01:28:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 8900/22000, train/total_loss: 0.0049, train/total_loss/avg: 0.1856, train/hateful_memes/cross_entropy: 0.0049, train/hateful_memes/cross_entropy/avg: 0.1856, max mem: 6424.0, experiment: run, epoch: 34, num_updates: 8900, iterations: 8900, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 499ms, time_since_start: 02h 26m 09s 312ms, eta: 03h 24m 08s 403ms\n",
      "\u001b[32m2020-09-26T01:30:17 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:30:17 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:30:17 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T01:30:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, train/total_loss: 0.0036, train/total_loss/avg: 0.1835, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1835, max mem: 6424.0, experiment: run, epoch: 34, num_updates: 9000, iterations: 9000, max_updates: 22000, lr: 0.00001, ups: 1.01, time: 01m 39s 529ms, time_since_start: 02h 27m 48s 842ms, eta: 03h 35m 38s 856ms\n",
      "\u001b[32m2020-09-26T01:30:23 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:30:32 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:30:32 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T01:30:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9000/22000, val/total_loss: 2.4457, val/hateful_memes/cross_entropy: 2.4457, val/hateful_memes/accuracy: 0.5340, val/hateful_memes/binary_f1: 0.3753, val/hateful_memes/roc_auc: 0.5055, num_updates: 9000, epoch: 34, iterations: 9000, max_updates: 22000, val_time: 35s 783ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T01:32:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9100/22000, train/total_loss: 0.0035, train/total_loss/avg: 0.1815, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1815, max mem: 6424.0, experiment: run, epoch: 35, num_updates: 9100, iterations: 9100, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 250ms, time_since_start: 02h 30m 05s 878ms, eta: 03h 37m 41s 346ms\n",
      "\u001b[32m2020-09-26T01:34:14 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9200/22000, train/total_loss: 0.0036, train/total_loss/avg: 0.1796, train/hateful_memes/cross_entropy: 0.0036, train/hateful_memes/cross_entropy/avg: 0.1796, max mem: 6424.0, experiment: run, epoch: 35, num_updates: 9200, iterations: 9200, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 606ms, time_since_start: 02h 31m 39s 485ms, eta: 03h 19m 41s 651ms\n",
      "\u001b[32m2020-09-26T01:35:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9300/22000, train/total_loss: 0.0035, train/total_loss/avg: 0.1777, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1777, max mem: 6424.0, experiment: run, epoch: 35, num_updates: 9300, iterations: 9300, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 580ms, time_since_start: 02h 33m 13s 065ms, eta: 03h 18m 04s 702ms\n",
      "\u001b[32m2020-09-26T01:37:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9400/22000, train/total_loss: 0.0035, train/total_loss/avg: 0.1759, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1759, max mem: 6424.0, experiment: run, epoch: 36, num_updates: 9400, iterations: 9400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 852ms, time_since_start: 02h 34m 48s 917ms, eta: 03h 21m 17s 368ms\n",
      "\u001b[32m2020-09-26T01:38:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9500/22000, train/total_loss: 0.0035, train/total_loss/avg: 0.1741, train/hateful_memes/cross_entropy: 0.0035, train/hateful_memes/cross_entropy/avg: 0.1741, max mem: 6424.0, experiment: run, epoch: 36, num_updates: 9500, iterations: 9500, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 518ms, time_since_start: 02h 36m 22s 436ms, eta: 03h 14m 49s 798ms\n",
      "\u001b[32m2020-09-26T01:40:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9600/22000, train/total_loss: 0.0034, train/total_loss/avg: 0.1723, train/hateful_memes/cross_entropy: 0.0034, train/hateful_memes/cross_entropy/avg: 0.1723, max mem: 6424.0, experiment: run, epoch: 37, num_updates: 9600, iterations: 9600, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 701ms, time_since_start: 02h 37m 58s 137ms, eta: 03h 17m 46s 959ms\n",
      "\u001b[32m2020-09-26T01:42:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9700/22000, train/total_loss: 0.0028, train/total_loss/avg: 0.1705, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1705, max mem: 6424.0, experiment: run, epoch: 37, num_updates: 9700, iterations: 9700, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 524ms, time_since_start: 02h 39m 31s 661ms, eta: 03h 11m 43s 479ms\n",
      "\u001b[32m2020-09-26T01:43:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9800/22000, train/total_loss: 0.0028, train/total_loss/avg: 0.1695, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1695, max mem: 6424.0, experiment: run, epoch: 37, num_updates: 9800, iterations: 9800, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 768ms, time_since_start: 02h 41m 05s 429ms, eta: 03h 10m 39s 699ms\n",
      "\u001b[32m2020-09-26T01:45:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 9900/22000, train/total_loss: 0.0023, train/total_loss/avg: 0.1678, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1678, max mem: 6424.0, experiment: run, epoch: 38, num_updates: 9900, iterations: 9900, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 580ms, time_since_start: 02h 42m 41s 010ms, eta: 03h 12m 45s 245ms\n",
      "\u001b[32m2020-09-26T01:46:48 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:46:48 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:46:48 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T01:46:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, train/total_loss: 0.0028, train/total_loss/avg: 0.1662, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1662, max mem: 6424.0, experiment: run, epoch: 38, num_updates: 10000, iterations: 10000, max_updates: 22000, lr: 0.00001, ups: 1.00, time: 01m 40s 778ms, time_since_start: 02h 44m 21s 788ms, eta: 03h 21m 33s 439ms\n",
      "\u001b[32m2020-09-26T01:46:56 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:47:05 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T01:47:05 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T01:47:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10000/22000, val/total_loss: 2.5925, val/hateful_memes/cross_entropy: 2.5925, val/hateful_memes/accuracy: 0.5160, val/hateful_memes/binary_f1: 0.3086, val/hateful_memes/roc_auc: 0.4960, num_updates: 10000, epoch: 38, iterations: 10000, max_updates: 22000, val_time: 37s 770ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T01:49:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10100/22000, train/total_loss: 0.0028, train/total_loss/avg: 0.1646, train/hateful_memes/cross_entropy: 0.0028, train/hateful_memes/cross_entropy/avg: 0.1646, max mem: 6424.0, experiment: run, epoch: 38, num_updates: 10100, iterations: 10100, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 256ms, time_since_start: 02h 46m 34s 850ms, eta: 03h 08m 55s 523ms\n",
      "\u001b[32m2020-09-26T01:50:45 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10200/22000, train/total_loss: 0.0026, train/total_loss/avg: 0.1630, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.1630, max mem: 6424.0, experiment: run, epoch: 39, num_updates: 10200, iterations: 10200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 445ms, time_since_start: 02h 48m 10s 296ms, eta: 03h 07m 42s 571ms\n",
      "\u001b[32m2020-09-26T01:52:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10300/22000, train/total_loss: 0.0023, train/total_loss/avg: 0.1614, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1614, max mem: 6424.0, experiment: run, epoch: 39, num_updates: 10300, iterations: 10300, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 228ms, time_since_start: 02h 49m 43s 525ms, eta: 03h 01m 47s 757ms\n",
      "\u001b[32m2020-09-26T01:53:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10400/22000, train/total_loss: 0.0026, train/total_loss/avg: 0.1600, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.1600, max mem: 6424.0, experiment: run, epoch: 40, num_updates: 10400, iterations: 10400, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 788ms, time_since_start: 02h 51m 19s 314ms, eta: 03h 05m 11s 515ms\n",
      "\u001b[32m2020-09-26T01:55:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10500/22000, train/total_loss: 0.0023, train/total_loss/avg: 0.1585, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1585, max mem: 6424.0, experiment: run, epoch: 40, num_updates: 10500, iterations: 10500, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 717ms, time_since_start: 02h 52m 53s 031ms, eta: 02h 59m 37s 554ms\n",
      "\u001b[32m2020-09-26T01:57:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10600/22000, train/total_loss: 0.0026, train/total_loss/avg: 0.1574, train/hateful_memes/cross_entropy: 0.0026, train/hateful_memes/cross_entropy/avg: 0.1574, max mem: 6424.0, experiment: run, epoch: 40, num_updates: 10600, iterations: 10600, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 533ms, time_since_start: 02h 54m 26s 565ms, eta: 02h 57m 42s 867ms\n",
      "\u001b[32m2020-09-26T01:58:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10700/22000, train/total_loss: 0.0023, train/total_loss/avg: 0.1560, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1560, max mem: 6424.0, experiment: run, epoch: 41, num_updates: 10700, iterations: 10700, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 779ms, time_since_start: 02h 56m 02s 345ms, eta: 03h 23s 124ms\n",
      "\u001b[32m2020-09-26T02:00:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10800/22000, train/total_loss: 0.0020, train/total_loss/avg: 0.1545, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1545, max mem: 6424.0, experiment: run, epoch: 41, num_updates: 10800, iterations: 10800, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 239ms, time_since_start: 02h 57m 35s 585ms, eta: 02h 54m 02s 837ms\n",
      "\u001b[32m2020-09-26T02:01:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 10900/22000, train/total_loss: 0.0023, train/total_loss/avg: 0.1535, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1535, max mem: 6424.0, experiment: run, epoch: 41, num_updates: 10900, iterations: 10900, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 129ms, time_since_start: 02h 59m 08s 715ms, eta: 02h 52m 17s 396ms\n",
      "\u001b[32m2020-09-26T02:03:18 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:03:18 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:03:18 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T02:03:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, train/total_loss: 0.0023, train/total_loss/avg: 0.1521, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1521, max mem: 6424.0, experiment: run, epoch: 42, num_updates: 11000, iterations: 11000, max_updates: 22000, lr: 0.00001, ups: 0.99, time: 01m 41s 919ms, time_since_start: 03h 50s 634ms, eta: 03h 06m 51s 160ms\n",
      "\u001b[32m2020-09-26T02:03:25 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:03:34 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:03:34 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T02:04:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11000/22000, val/total_loss: 2.6878, val/hateful_memes/cross_entropy: 2.6878, val/hateful_memes/accuracy: 0.5220, val/hateful_memes/binary_f1: 0.3416, val/hateful_memes/roc_auc: 0.4977, num_updates: 11000, epoch: 42, iterations: 11000, max_updates: 22000, val_time: 37s 693ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T02:05:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11100/22000, train/total_loss: 0.0023, train/total_loss/avg: 0.1507, train/hateful_memes/cross_entropy: 0.0023, train/hateful_memes/cross_entropy/avg: 0.1507, max mem: 6424.0, experiment: run, epoch: 42, num_updates: 11100, iterations: 11100, max_updates: 22000, lr: 0.00001, ups: 1.06, time: 01m 34s 660ms, time_since_start: 03h 03m 03s 306ms, eta: 02h 51m 58s 015ms\n",
      "\u001b[32m2020-09-26T02:07:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11200/22000, train/total_loss: 0.0020, train/total_loss/avg: 0.1494, train/hateful_memes/cross_entropy: 0.0020, train/hateful_memes/cross_entropy/avg: 0.1494, max mem: 6424.0, experiment: run, epoch: 43, num_updates: 11200, iterations: 11200, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 610ms, time_since_start: 03h 04m 38s 917ms, eta: 02h 52m 05s 990ms\n",
      "\u001b[32m2020-09-26T02:08:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11300/22000, train/total_loss: 0.0019, train/total_loss/avg: 0.1481, train/hateful_memes/cross_entropy: 0.0019, train/hateful_memes/cross_entropy/avg: 0.1481, max mem: 6424.0, experiment: run, epoch: 43, num_updates: 11300, iterations: 11300, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 323ms, time_since_start: 03h 06m 12s 241ms, eta: 02h 46m 25s 648ms\n",
      "\u001b[32m2020-09-26T02:10:20 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11400/22000, train/total_loss: 0.0016, train/total_loss/avg: 0.1468, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1468, max mem: 6424.0, experiment: run, epoch: 43, num_updates: 11400, iterations: 11400, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 075ms, time_since_start: 03h 07m 45s 316ms, eta: 02h 44m 25s 991ms\n",
      "\u001b[32m2020-09-26T02:11:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11500/22000, train/total_loss: 0.0016, train/total_loss/avg: 0.1455, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1455, max mem: 6424.0, experiment: run, epoch: 44, num_updates: 11500, iterations: 11500, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 618ms, time_since_start: 03h 09m 20s 935ms, eta: 02h 47m 19s 983ms\n",
      "\u001b[32m2020-09-26T02:13:28 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11600/22000, train/total_loss: 0.0016, train/total_loss/avg: 0.1445, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1445, max mem: 6424.0, experiment: run, epoch: 44, num_updates: 11600, iterations: 11600, max_updates: 22000, lr: 0.00001, ups: 1.09, time: 01m 32s 980ms, time_since_start: 03h 10m 53s 916ms, eta: 02h 41m 09s 995ms\n",
      "\u001b[32m2020-09-26T02:15:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11700/22000, train/total_loss: 0.0016, train/total_loss/avg: 0.1432, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1432, max mem: 6424.0, experiment: run, epoch: 44, num_updates: 11700, iterations: 11700, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 268ms, time_since_start: 03h 12m 27s 185ms, eta: 02h 40m 06s 698ms\n",
      "\u001b[32m2020-09-26T02:16:37 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11800/22000, train/total_loss: 0.0016, train/total_loss/avg: 0.1422, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1422, max mem: 6424.0, experiment: run, epoch: 45, num_updates: 11800, iterations: 11800, max_updates: 22000, lr: 0.00001, ups: 1.05, time: 01m 35s 247ms, time_since_start: 03h 14m 02s 432ms, eta: 02h 41m 55s 221ms\n",
      "\u001b[32m2020-09-26T02:18:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 11900/22000, train/total_loss: 0.0016, train/total_loss/avg: 0.1410, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1410, max mem: 6424.0, experiment: run, epoch: 45, num_updates: 11900, iterations: 11900, max_updates: 22000, lr: 0.00001, ups: 1.08, time: 01m 33s 145ms, time_since_start: 03h 15m 35s 578ms, eta: 02h 36m 47s 723ms\n",
      "\u001b[32m2020-09-26T02:19:45 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:19:45 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:19:45 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T02:19:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, train/total_loss: 0.0016, train/total_loss/avg: 0.1399, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1399, max mem: 6424.0, experiment: run, epoch: 46, num_updates: 12000, iterations: 12000, max_updates: 22000, lr: 0.00001, ups: 0.98, time: 01m 42s 260ms, time_since_start: 03h 17m 17s 838ms, eta: 02h 50m 26s 025ms\n",
      "\u001b[32m2020-09-26T02:19:52 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:20:01 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:20:01 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T02:20:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12000/22000, val/total_loss: 2.5216, val/hateful_memes/cross_entropy: 2.5216, val/hateful_memes/accuracy: 0.4860, val/hateful_memes/binary_f1: 0.3654, val/hateful_memes/roc_auc: 0.4754, num_updates: 12000, epoch: 46, iterations: 12000, max_updates: 22000, val_time: 34s 902ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T02:22:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12100/22000, train/total_loss: 0.0016, train/total_loss/avg: 0.1387, train/hateful_memes/cross_entropy: 0.0016, train/hateful_memes/cross_entropy/avg: 0.1387, max mem: 6424.0, experiment: run, epoch: 46, num_updates: 12100, iterations: 12100, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 941ms, time_since_start: 03h 19m 27s 923ms, eta: 02h 36m 39s 197ms\n",
      "\u001b[32m2020-09-26T02:23:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12200/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1376, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1376, max mem: 6424.0, experiment: run, epoch: 46, num_updates: 12200, iterations: 12200, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 377ms, time_since_start: 03h 21m 01s 300ms, eta: 02h 32m 30s 950ms\n",
      "\u001b[32m2020-09-26T02:25:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12300/22000, train/total_loss: 0.0014, train/total_loss/avg: 0.1365, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1365, max mem: 6424.0, experiment: run, epoch: 47, num_updates: 12300, iterations: 12300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 353ms, time_since_start: 03h 22m 36s 653ms, eta: 02h 34m 09s 273ms\n",
      "\u001b[32m2020-09-26T02:26:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12400/22000, train/total_loss: 0.0014, train/total_loss/avg: 0.1356, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1356, max mem: 6424.0, experiment: run, epoch: 47, num_updates: 12400, iterations: 12400, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 078ms, time_since_start: 03h 24m 09s 732ms, eta: 02h 28m 55s 547ms\n",
      "\u001b[32m2020-09-26T02:28:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12500/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1346, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1346, max mem: 6424.0, experiment: run, epoch: 47, num_updates: 12500, iterations: 12500, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 309ms, time_since_start: 03h 25m 43s 041ms, eta: 02h 27m 44s 388ms\n",
      "\u001b[32m2020-09-26T02:29:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12600/22000, train/total_loss: 0.0014, train/total_loss/avg: 0.1336, train/hateful_memes/cross_entropy: 0.0014, train/hateful_memes/cross_entropy/avg: 0.1336, max mem: 6424.0, experiment: run, epoch: 48, num_updates: 12600, iterations: 12600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 242ms, time_since_start: 03h 27m 18s 283ms, eta: 02h 29m 12s 771ms\n",
      "\u001b[32m2020-09-26T02:31:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12700/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1329, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1329, max mem: 6424.0, experiment: run, epoch: 48, num_updates: 12700, iterations: 12700, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 359ms, time_since_start: 03h 28m 51s 643ms, eta: 02h 24m 42s 454ms\n",
      "\u001b[32m2020-09-26T02:33:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12800/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1319, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1319, max mem: 6424.0, experiment: run, epoch: 49, num_updates: 12800, iterations: 12800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 634ms, time_since_start: 03h 30m 27s 277ms, eta: 02h 26m 38s 350ms\n",
      "\u001b[32m2020-09-26T02:34:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 12900/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1314, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1314, max mem: 6424.0, experiment: run, epoch: 49, num_updates: 12900, iterations: 12900, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 627ms, time_since_start: 03h 32m 905ms, eta: 02h 22m 124ms\n",
      "\u001b[32m2020-09-26T02:36:09 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:36:09 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:36:09 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T02:36:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1304, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1304, max mem: 6424.0, experiment: run, epoch: 49, num_updates: 13000, iterations: 13000, max_updates: 22000, lr: 0., ups: 1.01, time: 01m 39s 683ms, time_since_start: 03h 33m 40s 589ms, eta: 02h 29m 31s 509ms\n",
      "\u001b[32m2020-09-26T02:36:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T02:36:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13000/22000, val/total_loss: 2.8937, val/hateful_memes/cross_entropy: 2.8937, val/hateful_memes/accuracy: 0.4940, val/hateful_memes/binary_f1: 0.2792, val/hateful_memes/roc_auc: 0.4760, num_updates: 13000, epoch: 49, iterations: 13000, max_updates: 22000, val_time: 08s 695ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T02:38:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13100/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1294, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1294, max mem: 6424.0, experiment: run, epoch: 50, num_updates: 13100, iterations: 13100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 383ms, time_since_start: 03h 35m 25s 706ms, eta: 02h 22m 58s 156ms\n",
      "\u001b[32m2020-09-26T02:39:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13200/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1284, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1284, max mem: 6424.0, experiment: run, epoch: 50, num_updates: 13200, iterations: 13200, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 405ms, time_since_start: 03h 36m 59s 111ms, eta: 02h 16m 59s 689ms\n",
      "\u001b[32m2020-09-26T02:41:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13300/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1278, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1278, max mem: 6424.0, experiment: run, epoch: 50, num_updates: 13300, iterations: 13300, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 113ms, time_since_start: 03h 38m 32s 225ms, eta: 02h 15m 873ms\n",
      "\u001b[32m2020-09-26T02:42:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13400/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1268, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1268, max mem: 6424.0, experiment: run, epoch: 51, num_updates: 13400, iterations: 13400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 721ms, time_since_start: 03h 40m 07s 946ms, eta: 02h 17m 12s 027ms\n",
      "\u001b[32m2020-09-26T02:44:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13500/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1259, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1259, max mem: 6424.0, experiment: run, epoch: 51, num_updates: 13500, iterations: 13500, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 121ms, time_since_start: 03h 41m 41s 068ms, eta: 02h 11m 55s 334ms\n",
      "\u001b[32m2020-09-26T02:45:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13600/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1250, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1250, max mem: 6424.0, experiment: run, epoch: 52, num_updates: 13600, iterations: 13600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 410ms, time_since_start: 03h 43m 16s 478ms, eta: 02h 13m 34s 483ms\n",
      "\u001b[32m2020-09-26T02:47:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13700/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1241, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1241, max mem: 6424.0, experiment: run, epoch: 52, num_updates: 13700, iterations: 13700, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 497ms, time_since_start: 03h 44m 49s 976ms, eta: 02h 09m 20s 332ms\n",
      "\u001b[32m2020-09-26T02:48:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13800/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1232, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1232, max mem: 6424.0, experiment: run, epoch: 52, num_updates: 13800, iterations: 13800, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 413ms, time_since_start: 03h 46m 23s 390ms, eta: 02h 07m 39s 931ms\n",
      "\u001b[32m2020-09-26T02:50:33 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 13900/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1225, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1225, max mem: 6424.0, experiment: run, epoch: 53, num_updates: 13900, iterations: 13900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 373ms, time_since_start: 03h 47m 58s 764ms, eta: 02h 08m 45s 258ms\n",
      "\u001b[32m2020-09-26T02:52:06 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:52:06 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T02:52:06 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T02:52:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1218, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1218, max mem: 6424.0, experiment: run, epoch: 53, num_updates: 14000, iterations: 14000, max_updates: 22000, lr: 0., ups: 0.98, time: 01m 42s 217ms, time_since_start: 03h 49m 40s 982ms, eta: 02h 16m 17s 437ms\n",
      "\u001b[32m2020-09-26T02:52:15 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T02:52:24 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14000/22000, val/total_loss: 2.7454, val/hateful_memes/cross_entropy: 2.7454, val/hateful_memes/accuracy: 0.4900, val/hateful_memes/binary_f1: 0.3478, val/hateful_memes/roc_auc: 0.4769, num_updates: 14000, epoch: 53, iterations: 14000, max_updates: 22000, val_time: 08s 462ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T02:54:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14100/22000, train/total_loss: 0.0012, train/total_loss/avg: 0.1209, train/hateful_memes/cross_entropy: 0.0012, train/hateful_memes/cross_entropy/avg: 0.1209, max mem: 6424.0, experiment: run, epoch: 54, num_updates: 14100, iterations: 14100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 361ms, time_since_start: 03h 51m 26s 018ms, eta: 02h 06m 52s 568ms\n",
      "\u001b[32m2020-09-26T02:55:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14200/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1201, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1201, max mem: 6424.0, experiment: run, epoch: 54, num_updates: 14200, iterations: 14200, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 244ms, time_since_start: 03h 52m 59s 263ms, eta: 02h 01m 13s 067ms\n",
      "\u001b[32m2020-09-26T02:57:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14300/22000, train/total_loss: 0.0017, train/total_loss/avg: 0.1194, train/hateful_memes/cross_entropy: 0.0017, train/hateful_memes/cross_entropy/avg: 0.1194, max mem: 6424.0, experiment: run, epoch: 54, num_updates: 14300, iterations: 14300, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 390ms, time_since_start: 03h 54m 32s 654ms, eta: 01h 59m 51s 103ms\n",
      "\u001b[32m2020-09-26T02:58:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14400/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1186, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1186, max mem: 6424.0, experiment: run, epoch: 55, num_updates: 14400, iterations: 14400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 101ms, time_since_start: 03h 56m 07s 755ms, eta: 02h 27s 711ms\n",
      "\u001b[32m2020-09-26T03:00:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14500/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1180, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1180, max mem: 6424.0, experiment: run, epoch: 55, num_updates: 14500, iterations: 14500, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 212ms, time_since_start: 03h 57m 40s 967ms, eta: 01h 56m 30s 904ms\n",
      "\u001b[32m2020-09-26T03:01:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14600/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1172, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1172, max mem: 6424.0, experiment: run, epoch: 55, num_updates: 14600, iterations: 14600, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 155ms, time_since_start: 03h 59m 14s 123ms, eta: 01h 54m 53s 512ms\n",
      "\u001b[32m2020-09-26T03:03:23 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14700/22000, train/total_loss: 0.0013, train/total_loss/avg: 0.1164, train/hateful_memes/cross_entropy: 0.0013, train/hateful_memes/cross_entropy/avg: 0.1164, max mem: 6424.0, experiment: run, epoch: 56, num_updates: 14700, iterations: 14700, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 926ms, time_since_start: 04h 49s 050ms, eta: 01h 55m 29s 659ms\n",
      "\u001b[32m2020-09-26T03:04:56 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14800/22000, train/total_loss: 0.0011, train/total_loss/avg: 0.1156, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1156, max mem: 6424.0, experiment: run, epoch: 56, num_updates: 14800, iterations: 14800, max_updates: 22000, lr: 0., ups: 1.09, time: 01m 32s 977ms, time_since_start: 04h 02m 22s 028ms, eta: 01h 51m 34s 400ms\n",
      "\u001b[32m2020-09-26T03:06:31 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 14900/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1148, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1148, max mem: 6424.0, experiment: run, epoch: 57, num_updates: 14900, iterations: 14900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 057ms, time_since_start: 04h 03m 57s 085ms, eta: 01h 52m 29s 090ms\n",
      "\u001b[32m2020-09-26T03:08:05 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T03:08:05 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T03:08:05 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T03:08:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1141, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1141, max mem: 6424.0, experiment: run, epoch: 57, num_updates: 15000, iterations: 15000, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 839ms, time_since_start: 04h 05m 35s 925ms, eta: 01h 55m 18s 789ms\n",
      "\u001b[32m2020-09-26T03:08:11 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T03:08:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15000/22000, val/total_loss: 2.6088, val/hateful_memes/cross_entropy: 2.6088, val/hateful_memes/accuracy: 0.5040, val/hateful_memes/binary_f1: 0.3641, val/hateful_memes/roc_auc: 0.4868, num_updates: 15000, epoch: 57, iterations: 15000, max_updates: 22000, val_time: 08s 490ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T03:09:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15100/22000, train/total_loss: 0.0011, train/total_loss/avg: 0.1134, train/hateful_memes/cross_entropy: 0.0011, train/hateful_memes/cross_entropy/avg: 0.1134, max mem: 6424.0, experiment: run, epoch: 57, num_updates: 15100, iterations: 15100, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 973ms, time_since_start: 04h 07m 19s 099ms, eta: 01h 48m 04s 192ms\n",
      "\u001b[32m2020-09-26T03:11:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15200/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1127, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1127, max mem: 6424.0, experiment: run, epoch: 58, num_updates: 15200, iterations: 15200, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 360ms, time_since_start: 04h 08m 54s 459ms, eta: 01h 48m 04s 491ms\n",
      "\u001b[32m2020-09-26T03:13:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15300/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1119, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1119, max mem: 6424.0, experiment: run, epoch: 58, num_updates: 15300, iterations: 15300, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 260ms, time_since_start: 04h 10m 27s 720ms, eta: 01h 44m 08s 436ms\n",
      "\u001b[32m2020-09-26T03:14:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15400/22000, train/total_loss: 0.0015, train/total_loss/avg: 0.1112, train/hateful_memes/cross_entropy: 0.0015, train/hateful_memes/cross_entropy/avg: 0.1112, max mem: 6424.0, experiment: run, epoch: 58, num_updates: 15400, iterations: 15400, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 169ms, time_since_start: 04h 12m 889ms, eta: 01h 42m 29s 196ms\n",
      "\u001b[32m2020-09-26T03:16:10 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15500/22000, train/total_loss: 0.0010, train/total_loss/avg: 0.1105, train/hateful_memes/cross_entropy: 0.0010, train/hateful_memes/cross_entropy/avg: 0.1105, max mem: 6424.0, experiment: run, epoch: 59, num_updates: 15500, iterations: 15500, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 926ms, time_since_start: 04h 13m 35s 816ms, eta: 01h 42m 50s 251ms\n",
      "\u001b[32m2020-09-26T03:17:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15600/22000, train/total_loss: 0.0009, train/total_loss/avg: 0.1098, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1098, max mem: 6424.0, experiment: run, epoch: 59, num_updates: 15600, iterations: 15600, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 329ms, time_since_start: 04h 15m 09s 146ms, eta: 01h 39m 33s 109ms\n",
      "\u001b[32m2020-09-26T03:19:19 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15700/22000, train/total_loss: 0.0009, train/total_loss/avg: 0.1091, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1091, max mem: 6424.0, experiment: run, epoch: 60, num_updates: 15700, iterations: 15700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 211ms, time_since_start: 04h 16m 44s 357ms, eta: 01h 39m 58s 314ms\n",
      "\u001b[32m2020-09-26T03:20:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15800/22000, train/total_loss: 0.0009, train/total_loss/avg: 0.1084, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1084, max mem: 6424.0, experiment: run, epoch: 60, num_updates: 15800, iterations: 15800, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 422ms, time_since_start: 04h 18m 17s 780ms, eta: 01h 36m 32s 180ms\n",
      "\u001b[32m2020-09-26T03:22:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 15900/22000, train/total_loss: 0.0009, train/total_loss/avg: 0.1080, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1080, max mem: 6424.0, experiment: run, epoch: 60, num_updates: 15900, iterations: 15900, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 219ms, time_since_start: 04h 19m 50s 999ms, eta: 01h 34m 46s 371ms\n",
      "\u001b[32m2020-09-26T03:24:00 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T03:24:00 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T03:24:00 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T03:24:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, train/total_loss: 0.0009, train/total_loss/avg: 0.1074, train/hateful_memes/cross_entropy: 0.0009, train/hateful_memes/cross_entropy/avg: 0.1074, max mem: 6424.0, experiment: run, epoch: 61, num_updates: 16000, iterations: 16000, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 596ms, time_since_start: 04h 21m 32s 596ms, eta: 01h 41m 35s 804ms\n",
      "\u001b[32m2020-09-26T03:24:07 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T03:24:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16000/22000, val/total_loss: 2.7877, val/hateful_memes/cross_entropy: 2.7877, val/hateful_memes/accuracy: 0.4920, val/hateful_memes/binary_f1: 0.3520, val/hateful_memes/roc_auc: 0.4820, num_updates: 16000, epoch: 61, iterations: 16000, max_updates: 22000, val_time: 08s 259ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T03:25:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16100/22000, train/total_loss: 0.0007, train/total_loss/avg: 0.1067, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1067, max mem: 6424.0, experiment: run, epoch: 61, num_updates: 16100, iterations: 16100, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 936ms, time_since_start: 04h 23m 14s 793ms, eta: 01h 32m 22s 229ms\n",
      "\u001b[32m2020-09-26T03:27:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16200/22000, train/total_loss: 0.0006, train/total_loss/avg: 0.1061, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1061, max mem: 6424.0, experiment: run, epoch: 61, num_updates: 16200, iterations: 16200, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 305ms, time_since_start: 04h 24m 48s 098ms, eta: 01h 30m 11s 697ms\n",
      "\u001b[32m2020-09-26T03:28:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16300/22000, train/total_loss: 0.0006, train/total_loss/avg: 0.1055, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1055, max mem: 6424.0, experiment: run, epoch: 62, num_updates: 16300, iterations: 16300, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 949ms, time_since_start: 04h 26m 23s 047ms, eta: 01h 30m 12s 128ms\n",
      "\u001b[32m2020-09-26T03:30:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16400/22000, train/total_loss: 0.0007, train/total_loss/avg: 0.1049, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1049, max mem: 6424.0, experiment: run, epoch: 62, num_updates: 16400, iterations: 16400, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 117ms, time_since_start: 04h 27m 56s 165ms, eta: 01h 26m 54s 585ms\n",
      "\u001b[32m2020-09-26T03:32:06 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16500/22000, train/total_loss: 0.0007, train/total_loss/avg: 0.1043, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1043, max mem: 6424.0, experiment: run, epoch: 63, num_updates: 16500, iterations: 16500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 287ms, time_since_start: 04h 29m 31s 453ms, eta: 01h 27m 20s 820ms\n",
      "\u001b[32m2020-09-26T03:33:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16600/22000, train/total_loss: 0.0007, train/total_loss/avg: 0.1037, train/hateful_memes/cross_entropy: 0.0007, train/hateful_memes/cross_entropy/avg: 0.1037, max mem: 6424.0, experiment: run, epoch: 63, num_updates: 16600, iterations: 16600, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 293ms, time_since_start: 04h 31m 04s 746ms, eta: 01h 23m 57s 825ms\n",
      "\u001b[32m2020-09-26T03:35:12 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16700/22000, train/total_loss: 0.0006, train/total_loss/avg: 0.1030, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.1030, max mem: 6424.0, experiment: run, epoch: 63, num_updates: 16700, iterations: 16700, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 269ms, time_since_start: 04h 32m 38s 016ms, eta: 01h 22m 23s 296ms\n",
      "\u001b[32m2020-09-26T03:36:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16800/22000, train/total_loss: 0.0005, train/total_loss/avg: 0.1024, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1024, max mem: 6424.0, experiment: run, epoch: 64, num_updates: 16800, iterations: 16800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 395ms, time_since_start: 04h 34m 13s 411ms, eta: 01h 22m 40s 558ms\n",
      "\u001b[32m2020-09-26T03:38:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 16900/22000, train/total_loss: 0.0005, train/total_loss/avg: 0.1018, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.1018, max mem: 6424.0, experiment: run, epoch: 64, num_updates: 16900, iterations: 16900, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 212ms, time_since_start: 04h 35m 46s 624ms, eta: 01h 19m 13s 854ms\n",
      "\u001b[32m2020-09-26T03:39:54 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T03:39:54 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T03:39:54 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T03:39:59 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.1012, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1012, max mem: 6424.0, experiment: run, epoch: 64, num_updates: 17000, iterations: 17000, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 549ms, time_since_start: 04h 37m 25s 173ms, eta: 01h 22m 07s 455ms\n",
      "\u001b[32m2020-09-26T03:39:59 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T03:40:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17000/22000, val/total_loss: 2.8012, val/hateful_memes/cross_entropy: 2.8012, val/hateful_memes/accuracy: 0.5120, val/hateful_memes/binary_f1: 0.3776, val/hateful_memes/roc_auc: 0.4846, num_updates: 17000, epoch: 64, iterations: 17000, max_updates: 22000, val_time: 08s 501ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T03:41:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17100/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.1006, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1006, max mem: 6424.0, experiment: run, epoch: 65, num_updates: 17100, iterations: 17100, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 780ms, time_since_start: 04h 39m 09s 456ms, eta: 01h 18m 13s 239ms\n",
      "\u001b[32m2020-09-26T03:43:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17200/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.1001, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.1001, max mem: 6424.0, experiment: run, epoch: 65, num_updates: 17200, iterations: 17200, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 297ms, time_since_start: 04h 40m 42s 753ms, eta: 01h 14m 38s 279ms\n",
      "\u001b[32m2020-09-26T03:44:52 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17300/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0995, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0995, max mem: 6424.0, experiment: run, epoch: 66, num_updates: 17300, iterations: 17300, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 942ms, time_since_start: 04h 42m 17s 696ms, eta: 01h 14m 22s 298ms\n",
      "\u001b[32m2020-09-26T03:46:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17400/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0989, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0989, max mem: 6424.0, experiment: run, epoch: 66, num_updates: 17400, iterations: 17400, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 333ms, time_since_start: 04h 43m 51s 029ms, eta: 01h 11m 33s 346ms\n",
      "\u001b[32m2020-09-26T03:47:58 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17500/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0983, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0983, max mem: 6424.0, experiment: run, epoch: 66, num_updates: 17500, iterations: 17500, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 195ms, time_since_start: 04h 45m 24s 225ms, eta: 01h 09m 53s 806ms\n",
      "\u001b[32m2020-09-26T03:49:34 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17600/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0978, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0978, max mem: 6424.0, experiment: run, epoch: 67, num_updates: 17600, iterations: 17600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 112ms, time_since_start: 04h 46m 59s 338ms, eta: 01h 09m 44s 960ms\n",
      "\u001b[32m2020-09-26T03:51:07 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17700/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0972, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0972, max mem: 6424.0, experiment: run, epoch: 67, num_updates: 17700, iterations: 17700, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 278ms, time_since_start: 04h 48m 32s 616ms, eta: 01h 06m 50s 965ms\n",
      "\u001b[32m2020-09-26T03:52:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17800/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0967, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0967, max mem: 6424.0, experiment: run, epoch: 67, num_updates: 17800, iterations: 17800, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 237ms, time_since_start: 04h 50m 05s 854ms, eta: 01h 05m 15s 980ms\n",
      "\u001b[32m2020-09-26T03:54:15 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 17900/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0962, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0962, max mem: 6424.0, experiment: run, epoch: 68, num_updates: 17900, iterations: 17900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 308ms, time_since_start: 04h 51m 41s 162ms, eta: 01h 05m 07s 661ms\n",
      "\u001b[32m2020-09-26T03:55:49 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T03:55:49 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T03:55:49 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T03:55:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0957, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0957, max mem: 6424.0, experiment: run, epoch: 68, num_updates: 18000, iterations: 18000, max_updates: 22000, lr: 0., ups: 1.01, time: 01m 39s 479ms, time_since_start: 04h 53m 20s 642ms, eta: 01h 06m 19s 193ms\n",
      "\u001b[32m2020-09-26T03:55:55 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T03:56:03 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18000/22000, val/total_loss: 2.8767, val/hateful_memes/cross_entropy: 2.8767, val/hateful_memes/accuracy: 0.5000, val/hateful_memes/binary_f1: 0.3622, val/hateful_memes/roc_auc: 0.4858, num_updates: 18000, epoch: 68, iterations: 18000, max_updates: 22000, val_time: 08s 409ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T03:57:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18100/22000, train/total_loss: 0.0005, train/total_loss/avg: 0.0951, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0951, max mem: 6424.0, experiment: run, epoch: 69, num_updates: 18100, iterations: 18100, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 955ms, time_since_start: 04h 55m 05s 014ms, eta: 01h 02m 22s 261ms\n",
      "\u001b[32m2020-09-26T03:59:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18200/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0946, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0946, max mem: 6424.0, experiment: run, epoch: 69, num_updates: 18200, iterations: 18200, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 397ms, time_since_start: 04h 56m 38s 411ms, eta: 59m 09s 095ms\n",
      "\u001b[32m2020-09-26T04:00:46 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18300/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0941, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0941, max mem: 6424.0, experiment: run, epoch: 69, num_updates: 18300, iterations: 18300, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 274ms, time_since_start: 04h 58m 11s 685ms, eta: 57m 31s 140ms\n",
      "\u001b[32m2020-09-26T04:02:21 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18400/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0936, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0936, max mem: 6424.0, experiment: run, epoch: 70, num_updates: 18400, iterations: 18400, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 220ms, time_since_start: 04h 59m 46s 906ms, eta: 57m 07s 945ms\n",
      "\u001b[32m2020-09-26T04:03:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18500/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0931, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0931, max mem: 6424.0, experiment: run, epoch: 70, num_updates: 18500, iterations: 18500, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 859ms, time_since_start: 05h 01m 20s 765ms, eta: 54m 45s 066ms\n",
      "\u001b[32m2020-09-26T04:05:29 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18600/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0926, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0926, max mem: 6424.0, experiment: run, epoch: 70, num_updates: 18600, iterations: 18600, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 662ms, time_since_start: 05h 02m 54s 427ms, eta: 53m 04s 518ms\n",
      "\u001b[32m2020-09-26T04:07:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18700/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0921, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0921, max mem: 6424.0, experiment: run, epoch: 71, num_updates: 18700, iterations: 18700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 564ms, time_since_start: 05h 04m 29s 992ms, eta: 52m 33s 640ms\n",
      "\u001b[32m2020-09-26T04:08:38 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18800/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0916, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0916, max mem: 6424.0, experiment: run, epoch: 71, num_updates: 18800, iterations: 18800, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 598ms, time_since_start: 05h 06m 03s 591ms, eta: 49m 55s 150ms\n",
      "\u001b[32m2020-09-26T04:10:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 18900/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0911, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0911, max mem: 6424.0, experiment: run, epoch: 72, num_updates: 18900, iterations: 18900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 205ms, time_since_start: 05h 07m 38s 797ms, eta: 49m 11s 375ms\n",
      "\u001b[32m2020-09-26T04:11:46 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T04:11:46 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T04:11:46 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T04:11:54 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0906, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0906, max mem: 6424.0, experiment: run, epoch: 72, num_updates: 19000, iterations: 19000, max_updates: 22000, lr: 0., ups: 1.00, time: 01m 40s 489ms, time_since_start: 05h 09m 19s 286ms, eta: 50m 14s 691ms\n",
      "\u001b[32m2020-09-26T04:11:54 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T04:12:02 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19000/22000, val/total_loss: 2.8275, val/hateful_memes/cross_entropy: 2.8275, val/hateful_memes/accuracy: 0.4960, val/hateful_memes/binary_f1: 0.3438, val/hateful_memes/roc_auc: 0.4888, num_updates: 19000, epoch: 72, iterations: 19000, max_updates: 22000, val_time: 08s 549ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T04:13:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19100/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0902, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0902, max mem: 6424.0, experiment: run, epoch: 72, num_updates: 19100, iterations: 19100, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 028ms, time_since_start: 05h 11m 01s 865ms, eta: 45m 26s 817ms\n",
      "\u001b[32m2020-09-26T04:15:11 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19200/22000, train/total_loss: 0.0005, train/total_loss/avg: 0.0897, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0897, max mem: 6424.0, experiment: run, epoch: 73, num_updates: 19200, iterations: 19200, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 917ms, time_since_start: 05h 12m 36s 783ms, eta: 44m 17s 689ms\n",
      "\u001b[32m2020-09-26T04:16:44 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19300/22000, train/total_loss: 0.0005, train/total_loss/avg: 0.0892, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0892, max mem: 6424.0, experiment: run, epoch: 73, num_updates: 19300, iterations: 19300, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 299ms, time_since_start: 05h 14m 10s 083ms, eta: 41m 59s 099ms\n",
      "\u001b[32m2020-09-26T04:18:17 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19400/22000, train/total_loss: 0.0006, train/total_loss/avg: 0.0888, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0888, max mem: 6424.0, experiment: run, epoch: 73, num_updates: 19400, iterations: 19400, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 158ms, time_since_start: 05h 15m 43s 242ms, eta: 40m 22s 124ms\n",
      "\u001b[32m2020-09-26T04:19:53 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19500/22000, train/total_loss: 0.0005, train/total_loss/avg: 0.0883, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0883, max mem: 6424.0, experiment: run, epoch: 74, num_updates: 19500, iterations: 19500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 014ms, time_since_start: 05h 17m 18s 256ms, eta: 39m 35s 369ms\n",
      "\u001b[32m2020-09-26T04:21:26 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19600/22000, train/total_loss: 0.0005, train/total_loss/avg: 0.0879, train/hateful_memes/cross_entropy: 0.0005, train/hateful_memes/cross_entropy/avg: 0.0879, max mem: 6424.0, experiment: run, epoch: 74, num_updates: 19600, iterations: 19600, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 132ms, time_since_start: 05h 18m 51s 389ms, eta: 37m 15s 179ms\n",
      "\u001b[32m2020-09-26T04:23:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19700/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0874, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0874, max mem: 6424.0, experiment: run, epoch: 75, num_updates: 19700, iterations: 19700, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 321ms, time_since_start: 05h 20m 26s 710ms, eta: 36m 32s 393ms\n",
      "\u001b[32m2020-09-26T04:24:35 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19800/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0870, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0870, max mem: 6424.0, experiment: run, epoch: 75, num_updates: 19800, iterations: 19800, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 566ms, time_since_start: 05h 22m 277ms, eta: 34m 18s 468ms\n",
      "\u001b[32m2020-09-26T04:26:08 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 19900/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0866, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0866, max mem: 6424.0, experiment: run, epoch: 75, num_updates: 19900, iterations: 19900, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 382ms, time_since_start: 05h 23m 33s 660ms, eta: 32m 41s 039ms\n",
      "\u001b[32m2020-09-26T04:27:43 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T04:27:43 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T04:27:43 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T04:27:49 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0861, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0861, max mem: 6424.0, experiment: run, epoch: 76, num_updates: 20000, iterations: 20000, max_updates: 22000, lr: 0., ups: 0.99, time: 01m 41s 246ms, time_since_start: 05h 25m 14s 906ms, eta: 33m 44s 923ms\n",
      "\u001b[32m2020-09-26T04:27:49 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T04:27:57 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20000/22000, val/total_loss: 3.1469, val/hateful_memes/cross_entropy: 3.1469, val/hateful_memes/accuracy: 0.5120, val/hateful_memes/binary_f1: 0.3107, val/hateful_memes/roc_auc: 0.4920, num_updates: 20000, epoch: 76, iterations: 20000, max_updates: 22000, val_time: 08s 283ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T04:29:32 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20100/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0857, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0857, max mem: 6424.0, experiment: run, epoch: 76, num_updates: 20100, iterations: 20100, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 171ms, time_since_start: 05h 26m 57s 363ms, eta: 29m 49s 251ms\n",
      "\u001b[32m2020-09-26T04:31:05 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20200/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0853, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0853, max mem: 6424.0, experiment: run, epoch: 76, num_updates: 20200, iterations: 20200, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 319ms, time_since_start: 05h 28m 30s 682ms, eta: 27m 59s 749ms\n",
      "\u001b[32m2020-09-26T04:32:40 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20300/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0849, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0849, max mem: 6424.0, experiment: run, epoch: 77, num_updates: 20300, iterations: 20300, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 468ms, time_since_start: 05h 30m 06s 151ms, eta: 27m 02s 973ms\n",
      "\u001b[32m2020-09-26T04:34:13 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20400/22000, train/total_loss: 0.0006, train/total_loss/avg: 0.0845, train/hateful_memes/cross_entropy: 0.0006, train/hateful_memes/cross_entropy/avg: 0.0845, max mem: 6424.0, experiment: run, epoch: 77, num_updates: 20400, iterations: 20400, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 036ms, time_since_start: 05h 31m 39s 187ms, eta: 24m 48s 577ms\n",
      "\u001b[32m2020-09-26T04:35:48 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20500/22000, train/total_loss: 0.0004, train/total_loss/avg: 0.0841, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0841, max mem: 6424.0, experiment: run, epoch: 78, num_updates: 20500, iterations: 20500, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 000ms, time_since_start: 05h 33m 14s 188ms, eta: 23m 45s 014ms\n",
      "\u001b[32m2020-09-26T04:37:22 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20600/22000, train/total_loss: 0.0004, train/total_loss/avg: 0.0837, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0837, max mem: 6424.0, experiment: run, epoch: 78, num_updates: 20600, iterations: 20600, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 584ms, time_since_start: 05h 34m 47s 773ms, eta: 21m 50s 187ms\n",
      "\u001b[32m2020-09-26T04:38:55 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20700/22000, train/total_loss: 0.0004, train/total_loss/avg: 0.0833, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0833, max mem: 6424.0, experiment: run, epoch: 78, num_updates: 20700, iterations: 20700, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 256ms, time_since_start: 05h 36m 21s 029ms, eta: 20m 12s 328ms\n",
      "\u001b[32m2020-09-26T04:40:30 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20800/22000, train/total_loss: 0.0004, train/total_loss/avg: 0.0829, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0829, max mem: 6424.0, experiment: run, epoch: 79, num_updates: 20800, iterations: 20800, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 205ms, time_since_start: 05h 37m 56s 235ms, eta: 19m 02s 468ms\n",
      "\u001b[32m2020-09-26T04:42:04 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 20900/22000, train/total_loss: 0.0004, train/total_loss/avg: 0.0825, train/hateful_memes/cross_entropy: 0.0004, train/hateful_memes/cross_entropy/avg: 0.0825, max mem: 6424.0, experiment: run, epoch: 79, num_updates: 20900, iterations: 20900, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 286ms, time_since_start: 05h 39m 29s 522ms, eta: 17m 06s 155ms\n",
      "\u001b[32m2020-09-26T04:43:37 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T04:43:37 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T04:43:37 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T04:43:43 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, train/total_loss: 0.0003, train/total_loss/avg: 0.0821, train/hateful_memes/cross_entropy: 0.0003, train/hateful_memes/cross_entropy/avg: 0.0821, max mem: 6424.0, experiment: run, epoch: 79, num_updates: 21000, iterations: 21000, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 934ms, time_since_start: 05h 41m 08s 456ms, eta: 16m 29s 343ms\n",
      "\u001b[32m2020-09-26T04:43:43 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T04:43:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21000/22000, val/total_loss: 3.0140, val/hateful_memes/cross_entropy: 3.0140, val/hateful_memes/accuracy: 0.5140, val/hateful_memes/binary_f1: 0.3342, val/hateful_memes/roc_auc: 0.4898, num_updates: 21000, epoch: 79, iterations: 21000, max_updates: 22000, val_time: 08s 498ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T04:45:27 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21100/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0817, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0817, max mem: 6424.0, experiment: run, epoch: 80, num_updates: 21100, iterations: 21100, max_updates: 22000, lr: 0., ups: 1.04, time: 01m 36s 037ms, time_since_start: 05h 42m 52s 993ms, eta: 14m 24s 334ms\n",
      "\u001b[32m2020-09-26T04:47:01 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21200/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0813, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0813, max mem: 6424.0, experiment: run, epoch: 80, num_updates: 21200, iterations: 21200, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 380ms, time_since_start: 05h 44m 26s 373ms, eta: 12m 27s 043ms\n",
      "\u001b[32m2020-09-26T04:48:36 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21300/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0809, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0809, max mem: 6424.0, experiment: run, epoch: 81, num_updates: 21300, iterations: 21300, max_updates: 22000, lr: 0., ups: 1.06, time: 01m 34s 929ms, time_since_start: 05h 46m 01s 303ms, eta: 11m 04s 508ms\n",
      "\u001b[32m2020-09-26T04:50:09 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21400/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0806, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0806, max mem: 6424.0, experiment: run, epoch: 81, num_updates: 21400, iterations: 21400, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 400ms, time_since_start: 05h 47m 34s 704ms, eta: 09m 20s 403ms\n",
      "\u001b[32m2020-09-26T04:51:42 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21500/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0802, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0802, max mem: 6424.0, experiment: run, epoch: 81, num_updates: 21500, iterations: 21500, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 271ms, time_since_start: 05h 49m 07s 975ms, eta: 07m 46s 357ms\n",
      "\u001b[32m2020-09-26T04:53:18 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21600/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0798, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0798, max mem: 6424.0, experiment: run, epoch: 82, num_updates: 21600, iterations: 21600, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 588ms, time_since_start: 05h 50m 43s 564ms, eta: 06m 22s 354ms\n",
      "\u001b[32m2020-09-26T04:54:51 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21700/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0795, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0795, max mem: 6424.0, experiment: run, epoch: 82, num_updates: 21700, iterations: 21700, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 395ms, time_since_start: 05h 52m 16s 960ms, eta: 04m 40s 187ms\n",
      "\u001b[32m2020-09-26T04:56:25 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21800/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0791, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0791, max mem: 6424.0, experiment: run, epoch: 82, num_updates: 21800, iterations: 21800, max_updates: 22000, lr: 0., ups: 1.08, time: 01m 33s 491ms, time_since_start: 05h 53m 50s 451ms, eta: 03m 06s 982ms\n",
      "\u001b[32m2020-09-26T04:58:00 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 21900/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0787, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0787, max mem: 6424.0, experiment: run, epoch: 83, num_updates: 21900, iterations: 21900, max_updates: 22000, lr: 0., ups: 1.05, time: 01m 35s 320ms, time_since_start: 05h 55m 25s 772ms, eta: 01m 35s 320ms\n",
      "\u001b[32m2020-09-26T04:59:33 | mmf.trainers.callbacks.checkpoint: \u001b[0mCheckpoint time. Saving a checkpoint.\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T04:59:33 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T04:59:33 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:200: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T04:59:39 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, train/total_loss: 0.0002, train/total_loss/avg: 0.0784, train/hateful_memes/cross_entropy: 0.0002, train/hateful_memes/cross_entropy/avg: 0.0784, max mem: 6424.0, experiment: run, epoch: 83, num_updates: 22000, iterations: 22000, max_updates: 22000, lr: 0., ups: 1.02, time: 01m 38s 879ms, time_since_start: 05h 57m 04s 651ms, eta: 0ms\n",
      "\u001b[32m2020-09-26T04:59:39 | mmf.trainers.core.training_loop: \u001b[0mEvaluation time. Running on full validation set...\n",
      "\u001b[32m2020-09-26T04:59:47 | mmf.trainers.callbacks.logistics: \u001b[0mprogress: 22000/22000, val/total_loss: 2.9803, val/hateful_memes/cross_entropy: 2.9803, val/hateful_memes/accuracy: 0.5040, val/hateful_memes/binary_f1: 0.3404, val/hateful_memes/roc_auc: 0.4875, num_updates: 22000, epoch: 83, iterations: 22000, max_updates: 22000, val_time: 08s 447ms, best_update: 8000, best_iteration: 8000, best_val/hateful_memes/roc_auc: 0.522768\n",
      "\u001b[32m2020-09-26T04:59:48 | mmf.trainers.core.training_loop: \u001b[0mStepping into final validation check\n",
      "\u001b[32m2020-09-26T04:59:48 | mmf.utils.checkpoint: \u001b[0mRestoring checkpoint\n",
      "\u001b[32m2020-09-26T04:59:48 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T05:00:08 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T05:00:08 | py.warnings: \u001b[0m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:218: UserWarning: Please also save or load the state of the optimzer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
      "\n",
      "\u001b[32m2020-09-26T05:00:08 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2020-09-26T05:00:08 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 8000\n",
      "\u001b[32m2020-09-26T05:00:08 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 8000\n",
      "\u001b[32m2020-09-26T05:00:08 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 31\n",
      "\u001b[32m2020-09-26T05:00:09 | mmf.trainers.mmf_trainer: \u001b[0mStarting inference on test set\n",
      "  0% 0/32 [00:00<?, ?it/s]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T05:00:11 | py.warnings: \u001b[0m/content/mmf/mmf/modules/losses.py:91: UserWarning: Sample list has not field 'targets', are you sure that your ImDB has labels? you may have wanted to run with evaluation.predict=true\n",
      "  \"Sample list has not field 'targets', are you \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T05:00:11 | py.warnings: \u001b[0m/content/mmf/mmf/modules/losses.py:91: UserWarning: Sample list has not field 'targets', are you sure that your ImDB has labels? you may have wanted to run with evaluation.predict=true\n",
      "  \"Sample list has not field 'targets', are you \"\n",
      "\n",
      "  3% 1/32 [00:01<00:57,  1.85s/it]\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T05:00:11 | py.warnings: \u001b[0m/content/mmf/mmf/common/report.py:82: UserWarning: targets not found in report. Metrics calculation might not work as expected.\n",
      "  + \"might not work as expected.\"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-09-26T05:00:11 | py.warnings: \u001b[0m/content/mmf/mmf/common/report.py:82: UserWarning: targets not found in report. Metrics calculation might not work as expected.\n",
      "  + \"might not work as expected.\"\n",
      "\n",
      "100% 32/32 [00:15<00:00,  2.09it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/mmf_run\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('mmf', 'console_scripts', 'mmf_run')())\n",
      "  File \"/content/mmf/mmf_cli/run.py\", line 122, in run\n",
      "    main(configuration, predict=predict)\n",
      "  File \"/content/mmf/mmf_cli/run.py\", line 56, in main\n",
      "    trainer.train()\n",
      "  File \"/content/mmf/mmf/trainers/mmf_trainer.py\", line 114, in train\n",
      "    self.inference()\n",
      "  File \"/content/mmf/mmf/trainers/mmf_trainer.py\", line 132, in inference\n",
      "    getattr(self, f\"{dataset}_loader\"), use_tqdm=True\n",
      "  File \"/content/mmf/mmf/trainers/core/evaluation_loop.py\", line 45, in evaluation_loop\n",
      "    combined_report.metrics = self.metrics(combined_report, combined_report)\n",
      "  File \"/content/mmf/mmf/modules/metrics.py\", line 145, in __call__\n",
      "    sample_list, model_output, *args, **kwargs\n",
      "  File \"/content/mmf/mmf/modules/metrics.py\", line 207, in _calculate_with_checks\n",
      "    value = self.calculate(*args, **kwargs)\n",
      "  File \"/content/mmf/mmf/modules/metrics.py\", line 234, in calculate\n",
      "    expected = sample_list[\"targets\"]\n",
      "KeyError: 'targets'\n"
     ]
    }
   ],
   "source": [
    "!mmf_run config='projects/hateful_memes/configs/visual_bert/from_coco.yaml' model='visual_bert' dataset=hateful_memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "TYr7wcyKCT5a",
    "outputId": "53c0c6c3-1869-48e1-c8d2-55ee99326963"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-25 16:14:51--  https://drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com/Lnmwdnq3YcF7F3YsJncp.zip?AWSAccessKeyId=AKIAJYJLFLA7N3WRICBQ\n",
      "Resolving drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com (drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com)... 52.218.228.235\n",
      "Connecting to drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com (drivendata-competition-fb-hateful-memes-data.s3.amazonaws.com)|52.218.228.235|:443... connected.\n",
      "HTTP request sent, awaiting response... 403 Forbidden\n",
      "2020-09-25 16:14:51 ERROR 403: Forbidden.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visual BERT COCO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veVLDVaJC7Qy",
    "outputId": "1fd84c24-82ff-4cda-a365-06051a8ef03c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/visual_bert/from_coco.yaml\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.utils.configuration: \u001b[0mOverriding option model to visual_bert\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to visual_bert.finetuned.hateful_memes.from_coco\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to True\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/visual_bert/from_coco.yaml', 'model=visual_bert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=visual_bert.finetuned.hateful_memes.from_coco', 'checkpoint.resume_pretrained=True', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'evaluation.predict=true'])\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf_cli.run: \u001b[0mUsing seed 47786181\n",
      "\u001b[32m2020-11-13T21:00:47 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2020-11-13T21:00:51 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of VisualBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.embeddings.token_type_embeddings_visual.weight', 'bert.embeddings.position_embeddings_visual.weight', 'bert.embeddings.projection.weight', 'bert.embeddings.projection.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2020-11-13T21:01:03 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2020-11-13T21:01:03 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2020-11-13T21:01:03 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:01:07 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:01:07 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:01:07 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-13T21:01:07 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.word_embeddings.weight from model.bert.embeddings.word_embeddings.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings.weight from model.bert.embeddings.position_embeddings.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings.weight from model.bert.embeddings.token_type_embeddings.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.weight from model.bert.embeddings.LayerNorm.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.LayerNorm.bias from model.bert.embeddings.LayerNorm.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.token_type_embeddings_visual.weight from model.bert.embeddings.token_type_embeddings_visual.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.position_embeddings_visual.weight from model.bert.embeddings.position_embeddings_visual.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.weight from model.bert.embeddings.projection.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.embeddings.projection.bias from model.bert.embeddings.projection.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.weight from model.bert.encoder.layer.0.attention.self.query.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.query.bias from model.bert.encoder.layer.0.attention.self.query.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.weight from model.bert.encoder.layer.0.attention.self.key.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.key.bias from model.bert.encoder.layer.0.attention.self.key.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.weight from model.bert.encoder.layer.0.attention.self.value.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.self.value.bias from model.bert.encoder.layer.0.attention.self.value.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.weight from model.bert.encoder.layer.0.attention.output.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.dense.bias from model.bert.encoder.layer.0.attention.output.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.weight from model.bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.attention.output.LayerNorm.bias from model.bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.weight from model.bert.encoder.layer.0.intermediate.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.intermediate.dense.bias from model.bert.encoder.layer.0.intermediate.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.weight from model.bert.encoder.layer.0.output.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.dense.bias from model.bert.encoder.layer.0.output.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.weight from model.bert.encoder.layer.0.output.LayerNorm.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.0.output.LayerNorm.bias from model.bert.encoder.layer.0.output.LayerNorm.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.weight from model.bert.encoder.layer.1.attention.self.query.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.query.bias from model.bert.encoder.layer.1.attention.self.query.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.weight from model.bert.encoder.layer.1.attention.self.key.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.key.bias from model.bert.encoder.layer.1.attention.self.key.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.weight from model.bert.encoder.layer.1.attention.self.value.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.self.value.bias from model.bert.encoder.layer.1.attention.self.value.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.weight from model.bert.encoder.layer.1.attention.output.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.dense.bias from model.bert.encoder.layer.1.attention.output.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.weight from model.bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.attention.output.LayerNorm.bias from model.bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.weight from model.bert.encoder.layer.1.intermediate.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.intermediate.dense.bias from model.bert.encoder.layer.1.intermediate.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.weight from model.bert.encoder.layer.1.output.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.dense.bias from model.bert.encoder.layer.1.output.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.weight from model.bert.encoder.layer.1.output.LayerNorm.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.1.output.LayerNorm.bias from model.bert.encoder.layer.1.output.LayerNorm.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.weight from model.bert.encoder.layer.2.attention.self.query.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.query.bias from model.bert.encoder.layer.2.attention.self.query.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.weight from model.bert.encoder.layer.2.attention.self.key.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.key.bias from model.bert.encoder.layer.2.attention.self.key.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.weight from model.bert.encoder.layer.2.attention.self.value.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.self.value.bias from model.bert.encoder.layer.2.attention.self.value.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.weight from model.bert.encoder.layer.2.attention.output.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.dense.bias from model.bert.encoder.layer.2.attention.output.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.weight from model.bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.attention.output.LayerNorm.bias from model.bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.weight from model.bert.encoder.layer.2.intermediate.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.intermediate.dense.bias from model.bert.encoder.layer.2.intermediate.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.weight from model.bert.encoder.layer.2.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.dense.bias from model.bert.encoder.layer.2.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.weight from model.bert.encoder.layer.2.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.2.output.LayerNorm.bias from model.bert.encoder.layer.2.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.weight from model.bert.encoder.layer.3.attention.self.query.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.query.bias from model.bert.encoder.layer.3.attention.self.query.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.weight from model.bert.encoder.layer.3.attention.self.key.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.key.bias from model.bert.encoder.layer.3.attention.self.key.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.weight from model.bert.encoder.layer.3.attention.self.value.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.self.value.bias from model.bert.encoder.layer.3.attention.self.value.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.weight from model.bert.encoder.layer.3.attention.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.dense.bias from model.bert.encoder.layer.3.attention.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.weight from model.bert.encoder.layer.3.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.attention.output.LayerNorm.bias from model.bert.encoder.layer.3.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.weight from model.bert.encoder.layer.3.intermediate.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.intermediate.dense.bias from model.bert.encoder.layer.3.intermediate.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.weight from model.bert.encoder.layer.3.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.dense.bias from model.bert.encoder.layer.3.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.weight from model.bert.encoder.layer.3.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.3.output.LayerNorm.bias from model.bert.encoder.layer.3.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.weight from model.bert.encoder.layer.4.attention.self.query.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.query.bias from model.bert.encoder.layer.4.attention.self.query.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.weight from model.bert.encoder.layer.4.attention.self.key.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.key.bias from model.bert.encoder.layer.4.attention.self.key.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.weight from model.bert.encoder.layer.4.attention.self.value.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.self.value.bias from model.bert.encoder.layer.4.attention.self.value.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.weight from model.bert.encoder.layer.4.attention.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.dense.bias from model.bert.encoder.layer.4.attention.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.weight from model.bert.encoder.layer.4.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.attention.output.LayerNorm.bias from model.bert.encoder.layer.4.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.weight from model.bert.encoder.layer.4.intermediate.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.intermediate.dense.bias from model.bert.encoder.layer.4.intermediate.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.weight from model.bert.encoder.layer.4.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.dense.bias from model.bert.encoder.layer.4.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.weight from model.bert.encoder.layer.4.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.4.output.LayerNorm.bias from model.bert.encoder.layer.4.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.weight from model.bert.encoder.layer.5.attention.self.query.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.query.bias from model.bert.encoder.layer.5.attention.self.query.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.weight from model.bert.encoder.layer.5.attention.self.key.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.key.bias from model.bert.encoder.layer.5.attention.self.key.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.weight from model.bert.encoder.layer.5.attention.self.value.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.self.value.bias from model.bert.encoder.layer.5.attention.self.value.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.weight from model.bert.encoder.layer.5.attention.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.dense.bias from model.bert.encoder.layer.5.attention.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.weight from model.bert.encoder.layer.5.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.attention.output.LayerNorm.bias from model.bert.encoder.layer.5.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.weight from model.bert.encoder.layer.5.intermediate.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.intermediate.dense.bias from model.bert.encoder.layer.5.intermediate.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.weight from model.bert.encoder.layer.5.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.dense.bias from model.bert.encoder.layer.5.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.weight from model.bert.encoder.layer.5.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.5.output.LayerNorm.bias from model.bert.encoder.layer.5.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.weight from model.bert.encoder.layer.6.attention.self.query.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.query.bias from model.bert.encoder.layer.6.attention.self.query.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.weight from model.bert.encoder.layer.6.attention.self.key.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.key.bias from model.bert.encoder.layer.6.attention.self.key.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.weight from model.bert.encoder.layer.6.attention.self.value.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.self.value.bias from model.bert.encoder.layer.6.attention.self.value.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.weight from model.bert.encoder.layer.6.attention.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.dense.bias from model.bert.encoder.layer.6.attention.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.weight from model.bert.encoder.layer.6.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.attention.output.LayerNorm.bias from model.bert.encoder.layer.6.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.weight from model.bert.encoder.layer.6.intermediate.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.intermediate.dense.bias from model.bert.encoder.layer.6.intermediate.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.weight from model.bert.encoder.layer.6.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.dense.bias from model.bert.encoder.layer.6.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.weight from model.bert.encoder.layer.6.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.6.output.LayerNorm.bias from model.bert.encoder.layer.6.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.weight from model.bert.encoder.layer.7.attention.self.query.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.query.bias from model.bert.encoder.layer.7.attention.self.query.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.weight from model.bert.encoder.layer.7.attention.self.key.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.key.bias from model.bert.encoder.layer.7.attention.self.key.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.weight from model.bert.encoder.layer.7.attention.self.value.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.self.value.bias from model.bert.encoder.layer.7.attention.self.value.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.weight from model.bert.encoder.layer.7.attention.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.dense.bias from model.bert.encoder.layer.7.attention.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.weight from model.bert.encoder.layer.7.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.attention.output.LayerNorm.bias from model.bert.encoder.layer.7.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.weight from model.bert.encoder.layer.7.intermediate.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.intermediate.dense.bias from model.bert.encoder.layer.7.intermediate.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.weight from model.bert.encoder.layer.7.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.dense.bias from model.bert.encoder.layer.7.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.weight from model.bert.encoder.layer.7.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.7.output.LayerNorm.bias from model.bert.encoder.layer.7.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.weight from model.bert.encoder.layer.8.attention.self.query.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.query.bias from model.bert.encoder.layer.8.attention.self.query.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.weight from model.bert.encoder.layer.8.attention.self.key.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.key.bias from model.bert.encoder.layer.8.attention.self.key.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.weight from model.bert.encoder.layer.8.attention.self.value.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.self.value.bias from model.bert.encoder.layer.8.attention.self.value.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.weight from model.bert.encoder.layer.8.attention.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.dense.bias from model.bert.encoder.layer.8.attention.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.weight from model.bert.encoder.layer.8.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.attention.output.LayerNorm.bias from model.bert.encoder.layer.8.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.weight from model.bert.encoder.layer.8.intermediate.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.intermediate.dense.bias from model.bert.encoder.layer.8.intermediate.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.weight from model.bert.encoder.layer.8.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.dense.bias from model.bert.encoder.layer.8.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.weight from model.bert.encoder.layer.8.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.8.output.LayerNorm.bias from model.bert.encoder.layer.8.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.weight from model.bert.encoder.layer.9.attention.self.query.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.query.bias from model.bert.encoder.layer.9.attention.self.query.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.weight from model.bert.encoder.layer.9.attention.self.key.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.key.bias from model.bert.encoder.layer.9.attention.self.key.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.weight from model.bert.encoder.layer.9.attention.self.value.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.self.value.bias from model.bert.encoder.layer.9.attention.self.value.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.weight from model.bert.encoder.layer.9.attention.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.dense.bias from model.bert.encoder.layer.9.attention.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.weight from model.bert.encoder.layer.9.attention.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.attention.output.LayerNorm.bias from model.bert.encoder.layer.9.attention.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.weight from model.bert.encoder.layer.9.intermediate.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.intermediate.dense.bias from model.bert.encoder.layer.9.intermediate.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.weight from model.bert.encoder.layer.9.output.dense.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.dense.bias from model.bert.encoder.layer.9.output.dense.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.weight from model.bert.encoder.layer.9.output.LayerNorm.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.9.output.LayerNorm.bias from model.bert.encoder.layer.9.output.LayerNorm.bias\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.weight from model.bert.encoder.layer.10.attention.self.query.weight\r\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.query.bias from model.bert.encoder.layer.10.attention.self.query.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.weight from model.bert.encoder.layer.10.attention.self.key.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.key.bias from model.bert.encoder.layer.10.attention.self.key.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.weight from model.bert.encoder.layer.10.attention.self.value.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.self.value.bias from model.bert.encoder.layer.10.attention.self.value.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.weight from model.bert.encoder.layer.10.attention.output.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.dense.bias from model.bert.encoder.layer.10.attention.output.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.weight from model.bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.attention.output.LayerNorm.bias from model.bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.weight from model.bert.encoder.layer.10.intermediate.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.intermediate.dense.bias from model.bert.encoder.layer.10.intermediate.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.weight from model.bert.encoder.layer.10.output.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.dense.bias from model.bert.encoder.layer.10.output.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.weight from model.bert.encoder.layer.10.output.LayerNorm.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.10.output.LayerNorm.bias from model.bert.encoder.layer.10.output.LayerNorm.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.weight from model.bert.encoder.layer.11.attention.self.query.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.query.bias from model.bert.encoder.layer.11.attention.self.query.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.weight from model.bert.encoder.layer.11.attention.self.key.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.key.bias from model.bert.encoder.layer.11.attention.self.key.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.weight from model.bert.encoder.layer.11.attention.self.value.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.self.value.bias from model.bert.encoder.layer.11.attention.self.value.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.weight from model.bert.encoder.layer.11.attention.output.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.dense.bias from model.bert.encoder.layer.11.attention.output.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.weight from model.bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.attention.output.LayerNorm.bias from model.bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.weight from model.bert.encoder.layer.11.intermediate.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.intermediate.dense.bias from model.bert.encoder.layer.11.intermediate.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.weight from model.bert.encoder.layer.11.output.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.dense.bias from model.bert.encoder.layer.11.output.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.weight from model.bert.encoder.layer.11.output.LayerNorm.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.encoder.layer.11.output.LayerNorm.bias from model.bert.encoder.layer.11.output.LayerNorm.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.weight from model.bert.pooler.dense.weight\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCopying model.bert.pooler.dense.bias from model.bert.pooler.dense.bias\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mPretrained model loaded\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n",
      "\u001b[32m2020-11-13T21:01:07 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100%|█████████████████████████████████████████████| 8/8 [01:02<00:00,  7.82s/it]\n",
      "\u001b[32m2020-11-13T21:02:10 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /home/ubuntu/save/hateful_memes_visual_bert_47786181/reports/hateful_memes_run_val_2020-11-13T21:02:10.csv\n",
      "\u001b[32m2020-11-13T21:02:10 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "!mmf_predict config='projects/hateful_memes/configs/visual_bert/from_coco.yaml' model='visual_bert' dataset=hateful_memes \\\n",
    "run_type=val checkpoint.resume_zoo='visual_bert.finetuned.hateful_memes.from_coco' checkpoint.resume_pretrained=False dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n",
    "dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtNMgZyOySVj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VilBert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/omegaconf/dictconfig.py:252: UserWarning: Keys with dot (model.bert) are deprecated and will have different semantic meaning the next major version of OmegaConf (2.1)\n",
      "See the compact keys issue for more details: https://github.com/omry/omegaconf/issues/152\n",
      "You can disable this warning by setting the environment variable OC_DISABLE_DOT_ACCESS_WARNING=1\n",
      "  warnings.warn(message=msg, category=UserWarning)\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.utils.configuration: \u001b[0mOverriding option config to projects/hateful_memes/configs/vilbert/from_cc.yaml\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.utils.configuration: \u001b[0mOverriding option model to vilbert\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.utils.configuration: \u001b[0mOverriding option datasets to hateful_memes\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.utils.configuration: \u001b[0mOverriding option run_type to val\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_zoo to vilbert.finetuned.hateful_memes.from_cc_original\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.utils.configuration: \u001b[0mOverriding option checkpoint.resume_pretrained to False\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.val[0] to hateful_memes/defaults/annotations/dev_seen.jsonl\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.utils.configuration: \u001b[0mOverriding option dataset_config.hateful_memes.annotations.test[0] to hateful_memes/defaults/annotations/test_seen.jsonl\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.utils.configuration: \u001b[0mOverriding option evaluation.predict to true\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf: \u001b[0mLogging to: ./save/train.log\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf_cli.run: \u001b[0mNamespace(config_override=None, local_rank=None, opts=['config=projects/hateful_memes/configs/vilbert/from_cc.yaml', 'model=vilbert', 'dataset=hateful_memes', 'run_type=val', 'checkpoint.resume_zoo=vilbert.finetuned.hateful_memes.from_cc_original', 'checkpoint.resume_pretrained=False', 'dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl', 'dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl', 'evaluation.predict=true'])\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf_cli.run: \u001b[0mTorch version: 1.6.0\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.utils.general: \u001b[0mCUDA Device 0 is: Tesla T4\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf_cli.run: \u001b[0mUsing seed 8636850\n",
      "\u001b[32m2020-11-14T00:32:08 | mmf.trainers.mmf_trainer: \u001b[0mLoading datasets\n",
      "\u001b[32m2020-11-14T00:32:12 | mmf.trainers.mmf_trainer: \u001b[0mLoading model\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing ViLBERTBase: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViLBERTBase from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing ViLBERTBase from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViLBERTBase were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.v_embeddings.image_embeddings.weight', 'bert.v_embeddings.image_embeddings.bias', 'bert.v_embeddings.image_location_embeddings.weight', 'bert.v_embeddings.image_location_embeddings.bias', 'bert.v_embeddings.LayerNorm.weight', 'bert.v_embeddings.LayerNorm.bias', 'bert.encoder.v_layer.0.attention.self.query.weight', 'bert.encoder.v_layer.0.attention.self.query.bias', 'bert.encoder.v_layer.0.attention.self.key.weight', 'bert.encoder.v_layer.0.attention.self.key.bias', 'bert.encoder.v_layer.0.attention.self.value.weight', 'bert.encoder.v_layer.0.attention.self.value.bias', 'bert.encoder.v_layer.0.attention.output.dense.weight', 'bert.encoder.v_layer.0.attention.output.dense.bias', 'bert.encoder.v_layer.0.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.0.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.0.intermediate.dense.weight', 'bert.encoder.v_layer.0.intermediate.dense.bias', 'bert.encoder.v_layer.0.output.dense.weight', 'bert.encoder.v_layer.0.output.dense.bias', 'bert.encoder.v_layer.0.output.LayerNorm.weight', 'bert.encoder.v_layer.0.output.LayerNorm.bias', 'bert.encoder.v_layer.1.attention.self.query.weight', 'bert.encoder.v_layer.1.attention.self.query.bias', 'bert.encoder.v_layer.1.attention.self.key.weight', 'bert.encoder.v_layer.1.attention.self.key.bias', 'bert.encoder.v_layer.1.attention.self.value.weight', 'bert.encoder.v_layer.1.attention.self.value.bias', 'bert.encoder.v_layer.1.attention.output.dense.weight', 'bert.encoder.v_layer.1.attention.output.dense.bias', 'bert.encoder.v_layer.1.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.1.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.1.intermediate.dense.weight', 'bert.encoder.v_layer.1.intermediate.dense.bias', 'bert.encoder.v_layer.1.output.dense.weight', 'bert.encoder.v_layer.1.output.dense.bias', 'bert.encoder.v_layer.1.output.LayerNorm.weight', 'bert.encoder.v_layer.1.output.LayerNorm.bias', 'bert.encoder.v_layer.2.attention.self.query.weight', 'bert.encoder.v_layer.2.attention.self.query.bias', 'bert.encoder.v_layer.2.attention.self.key.weight', 'bert.encoder.v_layer.2.attention.self.key.bias', 'bert.encoder.v_layer.2.attention.self.value.weight', 'bert.encoder.v_layer.2.attention.self.value.bias', 'bert.encoder.v_layer.2.attention.output.dense.weight', 'bert.encoder.v_layer.2.attention.output.dense.bias', 'bert.encoder.v_layer.2.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.2.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.2.intermediate.dense.weight', 'bert.encoder.v_layer.2.intermediate.dense.bias', 'bert.encoder.v_layer.2.output.dense.weight', 'bert.encoder.v_layer.2.output.dense.bias', 'bert.encoder.v_layer.2.output.LayerNorm.weight', 'bert.encoder.v_layer.2.output.LayerNorm.bias', 'bert.encoder.v_layer.3.attention.self.query.weight', 'bert.encoder.v_layer.3.attention.self.query.bias', 'bert.encoder.v_layer.3.attention.self.key.weight', 'bert.encoder.v_layer.3.attention.self.key.bias', 'bert.encoder.v_layer.3.attention.self.value.weight', 'bert.encoder.v_layer.3.attention.self.value.bias', 'bert.encoder.v_layer.3.attention.output.dense.weight', 'bert.encoder.v_layer.3.attention.output.dense.bias', 'bert.encoder.v_layer.3.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.3.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.3.intermediate.dense.weight', 'bert.encoder.v_layer.3.intermediate.dense.bias', 'bert.encoder.v_layer.3.output.dense.weight', 'bert.encoder.v_layer.3.output.dense.bias', 'bert.encoder.v_layer.3.output.LayerNorm.weight', 'bert.encoder.v_layer.3.output.LayerNorm.bias', 'bert.encoder.v_layer.4.attention.self.query.weight', 'bert.encoder.v_layer.4.attention.self.query.bias', 'bert.encoder.v_layer.4.attention.self.key.weight', 'bert.encoder.v_layer.4.attention.self.key.bias', 'bert.encoder.v_layer.4.attention.self.value.weight', 'bert.encoder.v_layer.4.attention.self.value.bias', 'bert.encoder.v_layer.4.attention.output.dense.weight', 'bert.encoder.v_layer.4.attention.output.dense.bias', 'bert.encoder.v_layer.4.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.4.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.4.intermediate.dense.weight', 'bert.encoder.v_layer.4.intermediate.dense.bias', 'bert.encoder.v_layer.4.output.dense.weight', 'bert.encoder.v_layer.4.output.dense.bias', 'bert.encoder.v_layer.4.output.LayerNorm.weight', 'bert.encoder.v_layer.4.output.LayerNorm.bias', 'bert.encoder.v_layer.5.attention.self.query.weight', 'bert.encoder.v_layer.5.attention.self.query.bias', 'bert.encoder.v_layer.5.attention.self.key.weight', 'bert.encoder.v_layer.5.attention.self.key.bias', 'bert.encoder.v_layer.5.attention.self.value.weight', 'bert.encoder.v_layer.5.attention.self.value.bias', 'bert.encoder.v_layer.5.attention.output.dense.weight', 'bert.encoder.v_layer.5.attention.output.dense.bias', 'bert.encoder.v_layer.5.attention.output.LayerNorm.weight', 'bert.encoder.v_layer.5.attention.output.LayerNorm.bias', 'bert.encoder.v_layer.5.intermediate.dense.weight', 'bert.encoder.v_layer.5.intermediate.dense.bias', 'bert.encoder.v_layer.5.output.dense.weight', 'bert.encoder.v_layer.5.output.dense.bias', 'bert.encoder.v_layer.5.output.LayerNorm.weight', 'bert.encoder.v_layer.5.output.LayerNorm.bias', 'bert.encoder.c_layer.0.biattention.query1.weight', 'bert.encoder.c_layer.0.biattention.query1.bias', 'bert.encoder.c_layer.0.biattention.key1.weight', 'bert.encoder.c_layer.0.biattention.key1.bias', 'bert.encoder.c_layer.0.biattention.value1.weight', 'bert.encoder.c_layer.0.biattention.value1.bias', 'bert.encoder.c_layer.0.biattention.query2.weight', 'bert.encoder.c_layer.0.biattention.query2.bias', 'bert.encoder.c_layer.0.biattention.key2.weight', 'bert.encoder.c_layer.0.biattention.key2.bias', 'bert.encoder.c_layer.0.biattention.value2.weight', 'bert.encoder.c_layer.0.biattention.value2.bias', 'bert.encoder.c_layer.0.biOutput.dense1.weight', 'bert.encoder.c_layer.0.biOutput.dense1.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.0.biOutput.q_dense1.weight', 'bert.encoder.c_layer.0.biOutput.q_dense1.bias', 'bert.encoder.c_layer.0.biOutput.dense2.weight', 'bert.encoder.c_layer.0.biOutput.dense2.bias', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.0.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.0.biOutput.q_dense2.weight', 'bert.encoder.c_layer.0.biOutput.q_dense2.bias', 'bert.encoder.c_layer.0.v_intermediate.dense.weight', 'bert.encoder.c_layer.0.v_intermediate.dense.bias', 'bert.encoder.c_layer.0.v_output.dense.weight', 'bert.encoder.c_layer.0.v_output.dense.bias', 'bert.encoder.c_layer.0.v_output.LayerNorm.weight', 'bert.encoder.c_layer.0.v_output.LayerNorm.bias', 'bert.encoder.c_layer.0.t_intermediate.dense.weight', 'bert.encoder.c_layer.0.t_intermediate.dense.bias', 'bert.encoder.c_layer.0.t_output.dense.weight', 'bert.encoder.c_layer.0.t_output.dense.bias', 'bert.encoder.c_layer.0.t_output.LayerNorm.weight', 'bert.encoder.c_layer.0.t_output.LayerNorm.bias', 'bert.encoder.c_layer.1.biattention.query1.weight', 'bert.encoder.c_layer.1.biattention.query1.bias', 'bert.encoder.c_layer.1.biattention.key1.weight', 'bert.encoder.c_layer.1.biattention.key1.bias', 'bert.encoder.c_layer.1.biattention.value1.weight', 'bert.encoder.c_layer.1.biattention.value1.bias', 'bert.encoder.c_layer.1.biattention.query2.weight', 'bert.encoder.c_layer.1.biattention.query2.bias', 'bert.encoder.c_layer.1.biattention.key2.weight', 'bert.encoder.c_layer.1.biattention.key2.bias', 'bert.encoder.c_layer.1.biattention.value2.weight', 'bert.encoder.c_layer.1.biattention.value2.bias', 'bert.encoder.c_layer.1.biOutput.dense1.weight', 'bert.encoder.c_layer.1.biOutput.dense1.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.1.biOutput.q_dense1.weight', 'bert.encoder.c_layer.1.biOutput.q_dense1.bias', 'bert.encoder.c_layer.1.biOutput.dense2.weight', 'bert.encoder.c_layer.1.biOutput.dense2.bias', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.1.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.1.biOutput.q_dense2.weight', 'bert.encoder.c_layer.1.biOutput.q_dense2.bias', 'bert.encoder.c_layer.1.v_intermediate.dense.weight', 'bert.encoder.c_layer.1.v_intermediate.dense.bias', 'bert.encoder.c_layer.1.v_output.dense.weight', 'bert.encoder.c_layer.1.v_output.dense.bias', 'bert.encoder.c_layer.1.v_output.LayerNorm.weight', 'bert.encoder.c_layer.1.v_output.LayerNorm.bias', 'bert.encoder.c_layer.1.t_intermediate.dense.weight', 'bert.encoder.c_layer.1.t_intermediate.dense.bias', 'bert.encoder.c_layer.1.t_output.dense.weight', 'bert.encoder.c_layer.1.t_output.dense.bias', 'bert.encoder.c_layer.1.t_output.LayerNorm.weight', 'bert.encoder.c_layer.1.t_output.LayerNorm.bias', 'bert.encoder.c_layer.2.biattention.query1.weight', 'bert.encoder.c_layer.2.biattention.query1.bias', 'bert.encoder.c_layer.2.biattention.key1.weight', 'bert.encoder.c_layer.2.biattention.key1.bias', 'bert.encoder.c_layer.2.biattention.value1.weight', 'bert.encoder.c_layer.2.biattention.value1.bias', 'bert.encoder.c_layer.2.biattention.query2.weight', 'bert.encoder.c_layer.2.biattention.query2.bias', 'bert.encoder.c_layer.2.biattention.key2.weight', 'bert.encoder.c_layer.2.biattention.key2.bias', 'bert.encoder.c_layer.2.biattention.value2.weight', 'bert.encoder.c_layer.2.biattention.value2.bias', 'bert.encoder.c_layer.2.biOutput.dense1.weight', 'bert.encoder.c_layer.2.biOutput.dense1.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.2.biOutput.q_dense1.weight', 'bert.encoder.c_layer.2.biOutput.q_dense1.bias', 'bert.encoder.c_layer.2.biOutput.dense2.weight', 'bert.encoder.c_layer.2.biOutput.dense2.bias', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.2.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.2.biOutput.q_dense2.weight', 'bert.encoder.c_layer.2.biOutput.q_dense2.bias', 'bert.encoder.c_layer.2.v_intermediate.dense.weight', 'bert.encoder.c_layer.2.v_intermediate.dense.bias', 'bert.encoder.c_layer.2.v_output.dense.weight', 'bert.encoder.c_layer.2.v_output.dense.bias', 'bert.encoder.c_layer.2.v_output.LayerNorm.weight', 'bert.encoder.c_layer.2.v_output.LayerNorm.bias', 'bert.encoder.c_layer.2.t_intermediate.dense.weight', 'bert.encoder.c_layer.2.t_intermediate.dense.bias', 'bert.encoder.c_layer.2.t_output.dense.weight', 'bert.encoder.c_layer.2.t_output.dense.bias', 'bert.encoder.c_layer.2.t_output.LayerNorm.weight', 'bert.encoder.c_layer.2.t_output.LayerNorm.bias', 'bert.encoder.c_layer.3.biattention.query1.weight', 'bert.encoder.c_layer.3.biattention.query1.bias', 'bert.encoder.c_layer.3.biattention.key1.weight', 'bert.encoder.c_layer.3.biattention.key1.bias', 'bert.encoder.c_layer.3.biattention.value1.weight', 'bert.encoder.c_layer.3.biattention.value1.bias', 'bert.encoder.c_layer.3.biattention.query2.weight', 'bert.encoder.c_layer.3.biattention.query2.bias', 'bert.encoder.c_layer.3.biattention.key2.weight', 'bert.encoder.c_layer.3.biattention.key2.bias', 'bert.encoder.c_layer.3.biattention.value2.weight', 'bert.encoder.c_layer.3.biattention.value2.bias', 'bert.encoder.c_layer.3.biOutput.dense1.weight', 'bert.encoder.c_layer.3.biOutput.dense1.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.3.biOutput.q_dense1.weight', 'bert.encoder.c_layer.3.biOutput.q_dense1.bias', 'bert.encoder.c_layer.3.biOutput.dense2.weight', 'bert.encoder.c_layer.3.biOutput.dense2.bias', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.3.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.3.biOutput.q_dense2.weight', 'bert.encoder.c_layer.3.biOutput.q_dense2.bias', 'bert.encoder.c_layer.3.v_intermediate.dense.weight', 'bert.encoder.c_layer.3.v_intermediate.dense.bias', 'bert.encoder.c_layer.3.v_output.dense.weight', 'bert.encoder.c_layer.3.v_output.dense.bias', 'bert.encoder.c_layer.3.v_output.LayerNorm.weight', 'bert.encoder.c_layer.3.v_output.LayerNorm.bias', 'bert.encoder.c_layer.3.t_intermediate.dense.weight', 'bert.encoder.c_layer.3.t_intermediate.dense.bias', 'bert.encoder.c_layer.3.t_output.dense.weight', 'bert.encoder.c_layer.3.t_output.dense.bias', 'bert.encoder.c_layer.3.t_output.LayerNorm.weight', 'bert.encoder.c_layer.3.t_output.LayerNorm.bias', 'bert.encoder.c_layer.4.biattention.query1.weight', 'bert.encoder.c_layer.4.biattention.query1.bias', 'bert.encoder.c_layer.4.biattention.key1.weight', 'bert.encoder.c_layer.4.biattention.key1.bias', 'bert.encoder.c_layer.4.biattention.value1.weight', 'bert.encoder.c_layer.4.biattention.value1.bias', 'bert.encoder.c_layer.4.biattention.query2.weight', 'bert.encoder.c_layer.4.biattention.query2.bias', 'bert.encoder.c_layer.4.biattention.key2.weight', 'bert.encoder.c_layer.4.biattention.key2.bias', 'bert.encoder.c_layer.4.biattention.value2.weight', 'bert.encoder.c_layer.4.biattention.value2.bias', 'bert.encoder.c_layer.4.biOutput.dense1.weight', 'bert.encoder.c_layer.4.biOutput.dense1.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.4.biOutput.q_dense1.weight', 'bert.encoder.c_layer.4.biOutput.q_dense1.bias', 'bert.encoder.c_layer.4.biOutput.dense2.weight', 'bert.encoder.c_layer.4.biOutput.dense2.bias', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.4.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.4.biOutput.q_dense2.weight', 'bert.encoder.c_layer.4.biOutput.q_dense2.bias', 'bert.encoder.c_layer.4.v_intermediate.dense.weight', 'bert.encoder.c_layer.4.v_intermediate.dense.bias', 'bert.encoder.c_layer.4.v_output.dense.weight', 'bert.encoder.c_layer.4.v_output.dense.bias', 'bert.encoder.c_layer.4.v_output.LayerNorm.weight', 'bert.encoder.c_layer.4.v_output.LayerNorm.bias', 'bert.encoder.c_layer.4.t_intermediate.dense.weight', 'bert.encoder.c_layer.4.t_intermediate.dense.bias', 'bert.encoder.c_layer.4.t_output.dense.weight', 'bert.encoder.c_layer.4.t_output.dense.bias', 'bert.encoder.c_layer.4.t_output.LayerNorm.weight', 'bert.encoder.c_layer.4.t_output.LayerNorm.bias', 'bert.encoder.c_layer.5.biattention.query1.weight', 'bert.encoder.c_layer.5.biattention.query1.bias', 'bert.encoder.c_layer.5.biattention.key1.weight', 'bert.encoder.c_layer.5.biattention.key1.bias', 'bert.encoder.c_layer.5.biattention.value1.weight', 'bert.encoder.c_layer.5.biattention.value1.bias', 'bert.encoder.c_layer.5.biattention.query2.weight', 'bert.encoder.c_layer.5.biattention.query2.bias', 'bert.encoder.c_layer.5.biattention.key2.weight', 'bert.encoder.c_layer.5.biattention.key2.bias', 'bert.encoder.c_layer.5.biattention.value2.weight', 'bert.encoder.c_layer.5.biattention.value2.bias', 'bert.encoder.c_layer.5.biOutput.dense1.weight', 'bert.encoder.c_layer.5.biOutput.dense1.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm1.bias', 'bert.encoder.c_layer.5.biOutput.q_dense1.weight', 'bert.encoder.c_layer.5.biOutput.q_dense1.bias', 'bert.encoder.c_layer.5.biOutput.dense2.weight', 'bert.encoder.c_layer.5.biOutput.dense2.bias', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.weight', 'bert.encoder.c_layer.5.biOutput.LayerNorm2.bias', 'bert.encoder.c_layer.5.biOutput.q_dense2.weight', 'bert.encoder.c_layer.5.biOutput.q_dense2.bias', 'bert.encoder.c_layer.5.v_intermediate.dense.weight', 'bert.encoder.c_layer.5.v_intermediate.dense.bias', 'bert.encoder.c_layer.5.v_output.dense.weight', 'bert.encoder.c_layer.5.v_output.dense.bias', 'bert.encoder.c_layer.5.v_output.LayerNorm.weight', 'bert.encoder.c_layer.5.v_output.LayerNorm.bias', 'bert.encoder.c_layer.5.t_intermediate.dense.weight', 'bert.encoder.c_layer.5.t_intermediate.dense.bias', 'bert.encoder.c_layer.5.t_output.dense.weight', 'bert.encoder.c_layer.5.t_output.dense.bias', 'bert.encoder.c_layer.5.t_output.LayerNorm.weight', 'bert.encoder.c_layer.5.t_output.LayerNorm.bias', 'bert.t_pooler.dense.weight', 'bert.t_pooler.dense.bias', 'bert.v_pooler.dense.weight', 'bert.v_pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[32m2020-11-14T00:32:19 | mmf.trainers.mmf_trainer: \u001b[0mLoading optimizer\n",
      "\u001b[32m2020-11-14T00:32:19 | mmf.trainers.mmf_trainer: \u001b[0mLoading metrics\n",
      "\u001b[32m2020-11-14T00:32:19 | mmf.utils.checkpoint: \u001b[0mLoading checkpoint\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-14T00:32:20 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-14T00:32:20 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-14T00:32:20 | mmf: \u001b[0mKey data_parallel is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-14T00:32:20 | mmf: \u001b[0mKey distributed is not present in registry, returning default value of None\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-14T00:32:21 | mmf.utils.checkpoint: \u001b[0mMissing keys ['model.bert.embeddings.position_ids'] in the checkpoint.\n",
      "If this is not your checkpoint, please open up an issue on MMF GitHub. \n",
      "Unexpected keys if any: []\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-14T00:32:21 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-14T00:32:21 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:255: UserWarning: 'optimizer' key is not present in the checkpoint asked to be loaded. Skipping.\n",
      "  \"'optimizer' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-14T00:32:21 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:298: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m2020-11-14T00:32:21 | py.warnings: \u001b[0m/home/ubuntu/mmf/mmf/utils/checkpoint.py:298: UserWarning: 'lr_scheduler' key is not present in the checkpoint asked to be loaded. Setting lr_scheduler's last_epoch to current_iteration.\n",
      "  \"'lr_scheduler' key is not present in the \"\n",
      "\n",
      "\u001b[32m2020-11-14T00:32:21 | mmf.utils.checkpoint: \u001b[0mCheckpoint loaded.\n",
      "\u001b[32m2020-11-14T00:32:21 | mmf.utils.checkpoint: \u001b[0mCurrent num updates: 0\n",
      "\u001b[32m2020-11-14T00:32:21 | mmf.utils.checkpoint: \u001b[0mCurrent iteration: 0\n",
      "\u001b[32m2020-11-14T00:32:21 | mmf.utils.checkpoint: \u001b[0mCurrent epoch: 0\n",
      "\u001b[32m2020-11-14T00:32:21 | mmf.trainers.core.evaluation_loop: \u001b[0mStarting val inference predictions\n",
      "\u001b[32m2020-11-14T00:32:21 | mmf.common.test_reporter: \u001b[0mPredicting for hateful_memes\n",
      "100%|███████████████████████████████████████████| 16/16 [01:00<00:00,  3.78s/it]\n",
      "\u001b[32m2020-11-14T00:33:22 | mmf.common.test_reporter: \u001b[0mWrote predictions for hateful_memes to /home/ubuntu/save/hateful_memes_vilbert_8636850/reports/hateful_memes_run_val_2020-11-14T00:33:22.csv\n",
      "\u001b[32m2020-11-14T00:33:22 | mmf.trainers.core.evaluation_loop: \u001b[0mFinished predicting\n"
     ]
    }
   ],
   "source": [
    "!mmf_predict config='projects/hateful_memes/configs/vilbert/from_cc.yaml' model='vilbert' dataset=hateful_memes \\\n",
    "run_type=val checkpoint.resume_zoo='vilbert.finetuned.hateful_memes.from_cc_original' checkpoint.resume_pretrained=False dataset_config.hateful_memes.annotations.val[0]=hateful_memes/defaults/annotations/dev_seen.jsonl \\\n",
    "dataset_config.hateful_memes.annotations.test[0]=hateful_memes/defaults/annotations/test_seen.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MMLD.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
